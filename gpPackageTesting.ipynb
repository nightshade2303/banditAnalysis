{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceacb821",
   "metadata": {},
   "source": [
    "# testing QL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5fd0937b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'L:/4portProb_processed/sessdf.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 17\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m plotSettings\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# import pickle \u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# with open('L:/4portProb_processed/cleandf.pkl', 'rb') as f:\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#     df = pickle.load(f)\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m sessdf \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mL:/4portProb_processed/sessdf.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m window \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# sessdf = df\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dlab\\miniforge3\\envs\\rishika\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dlab\\miniforge3\\envs\\rishika\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\dlab\\miniforge3\\envs\\rishika\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dlab\\miniforge3\\envs\\rishika\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\dlab\\miniforge3\\envs\\rishika\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'L:/4portProb_processed/sessdf.csv'"
     ]
    }
   ],
   "source": [
    "from utils.opconNosepokeFunctions import *\n",
    "from utils.supplementaryFunctions import *\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import entropy\n",
    "from scipy.stats import ttest_rel\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils import plotSettings\n",
    "\n",
    "# import pickle \n",
    "# with open('L:/4portProb_processed/cleandf.pkl', 'rb') as f:\n",
    "#     df = pickle.load(f)\n",
    "\n",
    "sessdf = pd.read_csv('L:/4portProb_processed/sessdf.csv')\n",
    "\n",
    "window = 5\n",
    "# sessdf = df\n",
    "sessdf['rr'] = sessdf.groupby(['animal', 'session'], as_index = False).reward.rolling(window, center=True).mean().reward\n",
    "bin_size = 50\n",
    "# sessdf['sess_bin'] = sessdf.groupby(['animal', 'task'])['session'].transform(lambda x: pd.cut(x, bins=range(0, x.max() + bin_size, bin_size), labels=False, right=False)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb01775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first folder - all data\n",
    "sessdf = sessdf[~(sessdf.animal == 'Raikou')]\n",
    "from utils.dfLoading import add_block_groups\n",
    "blocked_df = add_block_groups(sessdf)\n",
    "blocked_df = blocked_df.rename(columns = {'session':'block', 'sess_block': 'session'}).drop(columns = \"Unnamed: 0\")\n",
    "blocked_df['datetime'] = pd.to_datetime(blocked_df['datetime'])\n",
    "\n",
    "\n",
    "## week relative to each animal-task first date\n",
    "blocked_df['week_task'] = blocked_df.groupby(['animal', 'task'])['datetime'].transform(\n",
    "    lambda x: ((x - x.min()).dt.days // 7) + 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a90d049c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15135876 0.8        0.10196115 0.46431508]\n",
      "[0.8        0.46431508 0.46431508 0.15135876]\n",
      "[0.15135876 0.10196115 0.46431508 0.8       ]\n",
      "[0.46431508 0.8        0.46431508 0.15135876]\n",
      "[0.15135876 0.10196115 0.8        0.46431508]\n",
      "[0.46431508 0.8        0.15135876 0.46431508]\n",
      "[0.15135876 0.46431508 0.46431508 0.8       ]\n",
      "[0.46431508 0.8        0.10196115 0.15135876]\n",
      "[0.46431508 0.8        0.15135876 0.46431508]\n",
      "[0.15135876 0.46431508 0.46431508 0.8       ]\n",
      "[0.46431508 0.8        0.46431508 0.15135876]\n",
      "[0.46431508 0.10196115 0.15135876 0.8       ]\n",
      "[0.46431508 0.46431508 0.8        0.15135876]\n",
      "[0.10196115 0.46431508 0.15135876 0.8       ]\n",
      "[0.10196115 0.8        0.46431508 0.15135876]\n",
      "[0.8        0.10196115 0.46431508 0.15135876]\n",
      "[0.46431508 0.15135876 0.10196115 0.8       ]\n",
      "[0.15135876 0.46431508 0.46431508 0.8       ]\n",
      "[0.8        0.46431508 0.15135876 0.10196115]\n",
      "[0.8        0.15135876 0.46431508 0.46431508]\n",
      "[0.46431508 0.46431508 0.8        0.15135876]\n",
      "[0.8        0.46431508 0.10196115 0.15135876]\n",
      "[0.15135876 0.10196115 0.8        0.46431508]\n",
      "[0.46431508 0.8        0.10196115 0.15135876]\n",
      "[0.8        0.15135876 0.46431508 0.46431508]\n",
      "[0.15135876 0.46431508 0.46431508 0.8       ]\n",
      "[0.15135876 0.46431508 0.10196115 0.8       ]\n",
      "[0.46431508 0.8        0.46431508 0.15135876]\n",
      "[0.8        0.10196115 0.15135876 0.46431508]\n",
      "[0.46431508 0.46431508 0.15135876 0.8       ]\n",
      "[0.15135876 0.46431508 0.8        0.46431508]\n",
      "[0.15135876 0.8        0.46431508 0.10196115]\n",
      "[0.15135876 0.46431508 0.8        0.46431508]\n",
      "[0.10196115 0.15135876 0.46431508 0.8       ]\n",
      "[0.8        0.46431508 0.46431508 0.15135876]\n",
      "[0.8        0.10196115 0.15135876 0.46431508]\n",
      "[0.46431508 0.10196115 0.15135876 0.8       ]\n",
      "[0.15135876 0.46431508 0.46431508 0.8       ]\n",
      "[0.46431508 0.46431508 0.8        0.15135876]\n",
      "[0.46431508 0.46431508 0.8        0.15135876]\n",
      "[0.46431508 0.15135876 0.10196115 0.8       ]\n",
      "[0.46431508 0.8        0.15135876 0.46431508]\n",
      "[0.15135876 0.8        0.46431508 0.46431508]\n",
      "[0.10196115 0.46431508 0.8        0.15135876]\n",
      "[0.8        0.15135876 0.46431508 0.46431508]\n",
      "[0.10196115 0.8        0.15135876 0.46431508]\n",
      "[0.15135876 0.46431508 0.46431508 0.8       ]\n",
      "[0.15135876 0.46431508 0.8        0.46431508]\n",
      "[0.15135876 0.8        0.10196115 0.46431508]\n",
      "[0.15135876 0.8        0.46431508 0.10196115]\n",
      "[0.46431508 0.15135876 0.8        0.46431508]\n",
      "[0.15135876 0.46431508 0.8        0.46431508]\n",
      "[0.15135876 0.46431508 0.8        0.46431508]\n",
      "[0.10196115 0.15135876 0.46431508 0.8       ]\n",
      "[0.10196115 0.8        0.46431508 0.15135876]\n",
      "[0.15135876 0.10196115 0.8        0.46431508]\n",
      "[0.8        0.15135876 0.46431508 0.46431508]\n",
      "[0.15135876 0.8        0.10196115 0.46431508]\n",
      "[0.46431508 0.8        0.15135876 0.46431508]\n",
      "[0.15135876 0.8        0.10196115 0.46431508]\n",
      "[0.15135876 0.46431508 0.8        0.46431508]\n",
      "[0.46431508 0.10196115 0.8        0.15135876]\n",
      "[0.8        0.15135876 0.46431508 0.10196115]\n",
      "[0.10196115 0.8        0.46431508 0.15135876]\n",
      "[0.15135876 0.8        0.10196115 0.46431508]\n",
      "[0.46431508 0.8        0.46431508 0.15135876]\n",
      "[0.15135876 0.8        0.46431508 0.46431508]\n",
      "[0.46431508 0.15135876 0.46431508 0.8       ]\n",
      "[0.8        0.15135876 0.46431508 0.10196115]\n",
      "[0.10196115 0.8        0.15135876 0.46431508]\n",
      "[0.8        0.10196115 0.46431508 0.15135876]\n",
      "[0.46431508 0.15135876 0.10196115 0.8       ]\n",
      "[0.15135876 0.46431508 0.46431508 0.8       ]\n",
      "[0.15135876 0.8        0.10196115 0.46431508]\n",
      "[0.10196115 0.8        0.46431508 0.15135876]\n",
      "[0.15135876 0.8        0.46431508 0.10196115]\n",
      "[0.10196115 0.8        0.46431508 0.15135876]\n",
      "[0.15135876 0.8        0.46431508 0.46431508]\n",
      "[0.15135876 0.46431508 0.8        0.46431508]\n",
      "[0.46431508 0.46431508 0.15135876 0.8       ]\n",
      "[0.10196115 0.8        0.46431508 0.15135876]\n",
      "[0.15135876 0.46431508 0.10196115 0.8       ]\n",
      "[0.15135876 0.46431508 0.10196115 0.8       ]\n",
      "[0.46431508 0.15135876 0.8        0.46431508]\n",
      "[0.46431508 0.8        0.15135876 0.46431508]\n",
      "[0.15135876 0.46431508 0.46431508 0.8       ]\n",
      "[0.15135876 0.46431508 0.8        0.10196115]\n",
      "[0.8        0.46431508 0.15135876 0.46431508]\n",
      "[0.46431508 0.8        0.15135876 0.10196115]\n",
      "[0.46431508 0.8        0.10196115 0.15135876]\n",
      "[0.46431508 0.8        0.15135876 0.46431508]\n",
      "[0.46431508 0.46431508 0.15135876 0.8       ]\n",
      "[0.10196115 0.8        0.46431508 0.15135876]\n",
      "[0.10196115 0.8        0.15135876 0.46431508]\n",
      "[0.10196115 0.46431508 0.15135876 0.8       ]\n",
      "[0.10196115 0.46431508 0.8        0.15135876]\n",
      "[0.10196115 0.46431508 0.15135876 0.8       ]\n",
      "[0.46431508 0.15135876 0.10196115 0.8       ]\n",
      "[0.46431508 0.8        0.46431508 0.15135876]\n",
      "[0.8        0.15135876 0.10196115 0.46431508]\n"
     ]
    }
   ],
   "source": [
    "lr_u = 0.033\n",
    "lr_sig = 0.9128\n",
    "decay = 0.1106\n",
    "b = 0.\n",
    "tau = 1/500\n",
    "sticky = 0.8918\n",
    "q0 = 0\n",
    "dist_mat = abs(np.arange(4)[:, None] - np.arange(4)[None, :])\n",
    "\n",
    "def fxn(mean, arms, permute = False):\n",
    "    x = np.linspace(1, arms, arms)\n",
    "    sig = 1.75/2\n",
    "    amp = 0.7\n",
    "    vo = 0.1\n",
    "    gx = (amp*np.exp(-0.5*((x-mean)**2)/(sig**2)))+vo\n",
    "    if permute:\n",
    "        gx = np.random.permutation(gx)\n",
    "    return gx\n",
    "\n",
    "rounds = 100\n",
    "# l = [fxn(np.random.randint(1, 5), 4, True) for _ in range(10000)]\n",
    "\n",
    "p = np.zeros(shape = (rounds, 100, 4))\n",
    "q = np.zeros(shape = (rounds, 100, 4))\n",
    "a = np.zeros(shape = (rounds, 100), dtype = int)\n",
    "r = np.zeros(shape = (rounds, 100), dtype = int)\n",
    "for rnd in range(rounds):\n",
    "    q[rnd, 0, :] = np.ones(4)*q0\n",
    "    rp = l[rnd]\n",
    "    print (rp)\n",
    "    upper = 0\n",
    "    n_a = np.ones(4)\n",
    "    prev_a = np.zeros(4)\n",
    "\n",
    "    for t in range(100):\n",
    "        # decay q-val\n",
    "        # print ('q value', q[rnd, t, :])\n",
    "        q[rnd, t, :] = q[rnd, t, :] - (decay*q[rnd, t, :])\n",
    "        # print('decayed q ',q[rnd, t, :])\n",
    "\n",
    "        # softmax and choose\n",
    "        q_upper = q[rnd, t, :] + b*upper\n",
    "        p[rnd, t, :] = q_upper/tau + prev_a*sticky\n",
    "        # print('q+ucb*inv temp + sticky', p[rnd, t, :])\n",
    "        p[rnd, t, :] = p[rnd, t, :] - np.max(p[rnd, t, :])\n",
    "        # print('p pre-softmax', p[rnd, t, :])\n",
    "        p[rnd, t, :] = np.exp(p[rnd, t, :])\n",
    "        p[rnd, t, :] = p[rnd, t, :]/np.sum(p[rnd, t, :])\n",
    "        # print('p value ',p[rnd, t, :])\n",
    "        coin = np.random.multinomial(1, p[rnd, t, :])\n",
    "        a[rnd, t] = int(np.arange(4)[coin.nonzero()[0][0]])\n",
    "        # print('a ', a[rnd, t])\n",
    "        # increment n \n",
    "        n_a[a[rnd, t]] += 1\n",
    "\n",
    "        # reward\n",
    "        r[rnd, t] = np.random.choice(2, p = [1-rp[a[rnd, t]], rp[a[rnd, t]]])\n",
    "        # print('r', r[rnd, t])\n",
    "        if t<99:\n",
    "            # determine if locality is involved\n",
    "            if lr_sig > 0:\n",
    "                lr = lr_u * np.exp(- (dist_mat[a[rnd, t], :]**2) / (2 * (lr_sig**2)))\n",
    "                # print( 'lr', lr)\n",
    "                q[rnd, t+1, :] = q[rnd, t, :] + lr*(r[rnd, t] - q[rnd, t, :])\n",
    "                # print('current q-val', q[rnd, t, :])\n",
    "                # print('new q-val', q[rnd, t+1, :])\n",
    "            else:\n",
    "                # update q-val\n",
    "                q[rnd, t+1, a[rnd, t]] = q[rnd, t, a[rnd, t]] + lr*(r[rnd, t] - q[rnd, t, a[rnd, t]])\n",
    "            \n",
    "            q[rnd, t+1, :] = np.clip(q[rnd, t+1, :], 0, 1)\n",
    "            # print('clipped q-val', q[rnd, t+1, :])\n",
    "            upper = np.sqrt(2*np.log(t+1))/n_a\n",
    "            # print(upper)\n",
    "\n",
    "            # calc sticky param\n",
    "            prev_a = np.zeros(4)\n",
    "            prev_a[a[rnd, t]] += 1\n",
    "            # print('prev-a', prev_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30350610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nLL_ql_gen(x0, df, arms):\n",
    "    beta, tau, lr_u, lr_sig, decay, sticky, q0 = x0 \n",
    "    sessions = df.session.value_counts()\n",
    "    chosen_action = df.port.to_numpy().reshape(-1, 100)\n",
    "    rewarded = df.reward.to_numpy().reshape(-1, 100)\n",
    "    P = np.ones_like(rewarded)*1e-7\n",
    "    session_end = np.cumsum(sessions)\n",
    "    ll = 0\n",
    "    rounds = chosen_action.shape[0]\n",
    "    trials = chosen_action.shape[1]\n",
    "\n",
    "    dist_mat = abs(np.arange(arms)[:, None] - np.arange(arms)[None, :])\n",
    "    \n",
    "    q = np.zeros(shape = (rounds, trials, arms))\n",
    "\n",
    "    for rnd in range(rounds):\n",
    "        q[rnd, 0, :] = np.ones(arms)*q0\n",
    "        upper = 0\n",
    "        n_a = np.ones(arms)\n",
    "        prev_a = np.zeros(arms)\n",
    "\n",
    "        for t in range(1,trials):\n",
    "            # decay q-val\n",
    "            q[rnd, t, :] = q[rnd, t, :] - (decay*q[rnd, t, :])\n",
    "\n",
    "            # chosen_action\n",
    "            a = int(chosen_action[rnd, t]-1)\n",
    "\n",
    "            # increment n \n",
    "            n_a[a] += 1\n",
    "\n",
    "            # rewarded?\n",
    "            r = rewarded[rnd, t]\n",
    "            \n",
    "            # determine if locality is involved\n",
    "            if lr_sig > 0:\n",
    "                lr = lr_u * np.exp(-(dist_mat[a, :]**2) / (2 * (lr_sig**2)))\n",
    "                q[rnd, t, :] = q[rnd, t-1, :] + lr*(r - q[rnd, t-1, :])\n",
    "\n",
    "            else:\n",
    "                # update q-val\n",
    "                q[rnd, t, a] = q[rnd, t-1, a] + lr*(r - q[rnd, t-1, a])\n",
    "            \n",
    "            q[rnd, t, :] = np.clip(q[rnd, t, :], 0, 1)\n",
    "            upper = np.sqrt(2*np.log(t+1))/n_a\n",
    "\n",
    "            # calc sticky param\n",
    "            prev_a = np.zeros(arms)\n",
    "            prev_a[int(chosen_action[rnd, t-1]-1)] += 1\n",
    "            \n",
    "            # calculate softmax\n",
    "            q_upper = q[rnd, t, :] + beta*upper\n",
    "            p = q_upper/tau + prev_a*sticky\n",
    "            p = p - np.max(p)\n",
    "            p = np.exp(p)/np.sum(np.exp(p))\n",
    "            P[rnd, t] = p[a]\n",
    "\n",
    "    P[P == 1e-7] = np.nan\n",
    "    ll += np.nansum(np.log(P))\n",
    "    nll = -ll\n",
    "        \n",
    "    return nll, q, chosen_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ecd75a",
   "metadata": {},
   "source": [
    "# testing different GP packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbea1aa5",
   "metadata": {},
   "source": [
    "## load train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23516064",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load all data, divide into test-train\n",
    "# load data - first block only and separate into test-train 20-80\n",
    "def load_data_into_dict(seed_n = 42):\n",
    "    import pickle \n",
    "    with open('S:/fileTransferFromWindows/cleandf.pkl', 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "\n",
    "    # subset for expert performance\n",
    "    subset_df = df[(df.sess_bin>=4) & (df.task == 'unstr')]\n",
    "    from utils.dfLoading import subset_trials, add_block_groups\n",
    "    \n",
    "    # add block groups and select only first block \n",
    "    subset_df = add_block_groups(subset_df)\n",
    "    # subset_df = subset_df[subset_df.block_group==1]\n",
    "    trialsinsess = 100\n",
    "\n",
    "    # then subset for first 100 trials\n",
    "    subset_df = subset_trials(subset_df, trialsinsess=trialsinsess, head_trials = trialsinsess)\n",
    "\n",
    "    np.random.seed(seed_n)\n",
    "\n",
    "    test_sess_dict = {}\n",
    "    # pick 10% sessions for testing\n",
    "    for animal in subset_df.animal.unique():\n",
    "        unique_sess = subset_df[subset_df.animal == animal].session.nunique()\n",
    "        test_sess = np.random.choice(subset_df[subset_df.animal == animal].session.unique(), int(unique_sess*0.2), replace = False)\n",
    "        test_sess_dict[animal] = test_sess\n",
    "        subset_df.drop(subset_df[(subset_df.animal == animal) & (subset_df.session.isin(test_sess))].index, inplace = True)\n",
    "\n",
    "    sess_dict = {key: group for key, group in subset_df.groupby('animal')}\n",
    "\n",
    "    print('loaded data')\n",
    "    return sess_dict, test_sess_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58c3d2e",
   "metadata": {},
   "source": [
    "## sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4d14038",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sklearn\n",
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier     # generate GP \n",
    "from sklearn.gaussian_process.kernels import RBF, Product, ConstantKernel\n",
    "from scipy.special import expit\n",
    "from scipy.optimize import minimize\n",
    "# train_data, test_data = load_data_into_dict()\n",
    "\n",
    "def nLL_gp_ucb_sklearn(x0, df, arms):\n",
    "    beta, tau, ls = x0\n",
    "    ll = 0\n",
    "    sessions = df.session.value_counts()\n",
    "    chosen_action = df.port.to_numpy()\n",
    "    # chosen_action = np.concatenate([[1000, 1000], chosen_action])\n",
    "    rewarded = df.reward.to_numpy()\n",
    "    # rewarded = np.concatenate([[0, 1], rewarded])\n",
    "    p = np.ones(len(df))*1e-7\n",
    "    session_ends = np.cumsum(sessions)\n",
    "    \n",
    "    sess_num = 0\n",
    "    sess_start = True\n",
    "\n",
    "    # gp settings\n",
    "    X = np.linspace(1,arms,arms).reshape(arms,1)\n",
    "    knl = Product(RBF(length_scale=ls), ConstantKernel(constant_value=1**2))\n",
    "\n",
    "    for trial in range(len(chosen_action)):\n",
    "        # check if sess restarted\n",
    "        if sess_start == True:\n",
    "            # if group.block_group.unique()[0] == 1:\n",
    "            gp = GaussianProcessClassifier(kernel=knl, optimizer = None)\n",
    "            start_trial = trial\n",
    "\n",
    "        sess_start = False\n",
    "        \n",
    "        # if len(np.unique(rewarded[start_trial:trial])) > 1:\n",
    "        if trial - start_trial>1:\n",
    "            # what actions were taken so far\n",
    "            a = np.concatenate([[1000, 1000], chosen_action[start_trial:trial]])\n",
    "\n",
    "            # what rewards were given for each action\n",
    "            r = np.concatenate([[0, 1],rewarded[start_trial:trial]])\n",
    "            # print(a, r)\n",
    "            # update gp using all the info we got so far on actions and rew\n",
    "            gp.fit(a.reshape(-1, 1), r)\n",
    "\n",
    "            # get latent mean and variance\n",
    "            mu, var = gp.latent_mean_and_variance(X)\n",
    "            mu_star = expit(mu)\n",
    "            sd_star = expit(mu+np.sqrt(var)) - expit(mu)\n",
    "\n",
    "            # calculate probability of taking any action\n",
    "            input_to_softmax = (mu_star + beta*sd_star)/tau\n",
    "            P = np.exp(input_to_softmax - max(input_to_softmax))\n",
    "            P = P/ np.sum(P)\n",
    "            P = np.clip(P, a_min = 1e-7, a_max = None)\n",
    "\n",
    "            # what was probability of action taken?\n",
    "            chosen = int(chosen_action[trial]-1)\n",
    "            p[trial] = P[chosen]\n",
    "\n",
    "        # check how many trials elapsed\n",
    "        if trial == session_ends.iloc[sess_num]:\n",
    "            sess_start=True\n",
    "            sess_num += 1\n",
    "    p[p == 1e-7] = np.nan\n",
    "    ll += np.nansum(np.log(p))\n",
    "    nll = -ll\n",
    "    return nll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b17f0b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(7140.387636868272)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nLL_gp_ucb_sklearn([0.1, 1, 1], train_data['test05022023'], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ecb2b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-27.34294163,   1.03625047,   0.61151958])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_sklearn.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82d184d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_sklearn = minimize(nLL_gp_ucb_sklearn, [0.1, 1, 1], args = (train_data['test05022023'], 4), options = {'disp':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "582b1bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_sklearn2 = minimize(nLL_gp_ucb_sklearn, [0.1, 1, 1], args = (train_data['test05022023'], 4), options = {'disp':True}, method = 'L-BFGS-B', bounds = [(0, 500), (0.0015, 100), (1e-2, 1e+2)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af9adaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bads:TooCloseBounds: For each variable, hard and plausible bounds should not be too close. Moving plausible bounds.\n",
      "Initial starting point is invalid or not provided. Initial point randomly sampled uniformly from plausible box\n",
      "\n",
      "Variables (index) internally transformed to log coordinates: [[0 1]\n",
      " [0 2]]\n",
      "Beginning optimization of a DETERMINISTIC objective function\n",
      "\n",
      " Iteration    f-count         f(x)           MeshScale          Method             Actions\n",
      "     0           2         7473.64               1                                 Uncertainty test\n",
      "     0           6         7472.46               1         Initial mesh            Initial points\n",
      "     0           9         7472.26             0.5         Refine grid             Train\n",
      "     1          11         7472.13             0.5     Incremental search (ES-ell)        \n",
      "     1          12         7472.11             0.5     Incremental search (ES-wcm)        \n",
      "     1          13         7470.05             0.5     Successful search (ES-wcm)        \n",
      "     1          14         7243.95             0.5     Successful search (ES-ell)        \n",
      "     1          15         2154.05             0.5     Successful search (ES-ell)        \n",
      "     1          16         870.912             0.5     Successful search (ES-ell)        \n",
      "     1          18         836.384             0.5     Successful search (ES-ell)        \n",
      "     1          19         67.2003             0.5     Successful search (ES-ell)        \n",
      "     1          29         67.2003            0.25         Refine grid             Train\n",
      "     2          36         67.2003           0.125         Refine grid             Train\n",
      "     3          37         65.2467           0.125     Successful search (ES-ell)        \n",
      "     3          40         64.2974           0.125     Successful search (ES-ell)        \n",
      "     3          49         64.2974          0.0625         Refine grid             \n",
      "     4          52         64.2358          0.0625     Successful search (ES-ell)        \n",
      "     4          62         64.2358         0.03125         Refine grid             Train\n",
      "     5          65         64.2292         0.03125     Successful search (ES-ell)        \n",
      "     5          75         64.2292        0.015625         Refine grid             Train\n",
      "     6          76         64.1881        0.015625     Successful search (ES-ell)        \n",
      "     6          78         64.1868        0.015625     Incremental search (ES-ell)        \n",
      "     6          88         64.1868       0.0078125         Refine grid             Train\n",
      "     7          97         64.1868      0.00390625         Refine grid             Train\n",
      "     8          98         64.1861      0.00390625     Incremental search (ES-ell)        \n",
      "     8          99         64.1861      0.00390625     Incremental search (ES-wcm)        \n",
      "     8         106         64.1861      0.00195312         Refine grid             Train\n",
      "     9         115         64.1861     0.000488281         Refine grid             Train\n",
      "    10         116          64.186     0.000488281     Incremental search (ES-ell)        \n",
      "    10         118          64.186     0.000488281     Incremental search (ES-wcm)        \n",
      "    10         124          64.186      0.00012207         Refine grid             \n",
      "    11         125          64.186      0.00012207     Incremental search (ES-wcm)        \n",
      "Optimization terminated: change in the function value less than options['tol_fun'].\n",
      "Function value at minimum: 64.18604083640412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pybads import BADS\n",
    "\n",
    "def fun_for_pybads(x):\n",
    "    return nLL_gp_ucb_sklearn(x, train_data['test05022023'], 4)\n",
    "bads = BADS(fun_for_pybads, None, [0, 0.0015, 1e-2], [10, 100, 1e+2])\n",
    "opt_sklearn_bads = bads.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c3c0307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.71236328e+01, 1.50206880e-03, 1.00312876e-02])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_sklearn_bads.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7520fd73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.00150312, 0.01      ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_sklearn2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97749670",
   "metadata": {},
   "source": [
    "## GPJax (env not set up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccd537ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jax'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m## gpjax \u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# from utils.supplementaryFunctions import tic, toc\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjax\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mflax\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nnx\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjax\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'jax'"
     ]
    }
   ],
   "source": [
    "## gpjax \n",
    "\n",
    "# from utils.supplementaryFunctions import tic, toc\n",
    "import jax\n",
    "from flax import nnx\n",
    "\n",
    "from jax import config\n",
    "import jax.random as jr\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy as jsp\n",
    "import optax as ox\n",
    "import numpy as np \n",
    "import numpyro.distributions as npd\n",
    "\n",
    "config.update(\"jax_enable_x64\", True) ## Enable 64-bit float precision for jax. \n",
    "\n",
    "import gpjax as gpx\n",
    "from gpjax.linalg import PSD, lower_cholesky, solve\n",
    "from gpjax.parameters import Parameter\n",
    "\n",
    "# import dill as pickle \n",
    "from pybads import BADS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def _setup_GP(num_datatpoints, hyperparams):\n",
    "    if 'lengthscale' in hyperparams.keys():\n",
    "        kernel = gpx.kernels.RBF(lengthscale = hyperparams['lengthscale'])\n",
    "    else:\n",
    "        kernel = gpx.kernels.RBF()\n",
    "    meanf = gpx.mean_functions.Constant()\n",
    "    prior = gpx.gps.Prior(mean_function = meanf, kernel = kernel)\n",
    "    likelihood = gpx.likelihoods.Bernoulli(num_datapoints = num_datatpoints)\n",
    "    posterior = prior * likelihood\n",
    "    return likelihood, posterior\n",
    "\n",
    "def _compute_laplace(posterior, X_train, X_test, D, jitter = 1e-6):\n",
    "    \"\"\"\n",
    "    Modified from here, \n",
    "    \n",
    "    \"\"\"\n",
    "    # Compute the latent function value map at each of the training points. \n",
    "    # Kxx = posterior.prior.kernel.gram(X_train)\n",
    "    # Kxx += jnp.eye(D.n) * jitter \n",
    "    # Kxx = PSD(Kxx) # Forces positive-semi definite \n",
    "    # Lx = lower_cholesky(Kxx)\n",
    "    # f_hat = Lx @ posterior.latent.value # Compute the mean of the posterior distribution. \n",
    "\n",
    "    # Now we compute the negative Hessian, using autodifferentiation. \n",
    "    graphdef, params, *static_state = nnx.split(posterior, Parameter, ...)\n",
    "\n",
    "    def loss(params, D):\n",
    "        model = nnx.merge(graphdef, params, *static_state)\n",
    "        return -gpx.objectives.log_posterior_density(model, D)\n",
    "    \n",
    "    jacobian = jax.jacfwd(jax.jacrev(loss))(params, D)\n",
    "    H = jacobian[\"latent\"].value[\"latent\"].value[:, 0, :, 0]\n",
    "    L = jnp.linalg.cholesky(H + jnp.eye(D.n) * jitter)\n",
    "\n",
    "    # Compute H-1 \n",
    "    L_inv = jsp.linalg.solve_triangular(L, jnp.eye(D.n), lower = True)\n",
    "    H_inv = jsp.linalg.solve_triangular(L.T, L_inv, lower = False)\n",
    "    ## Do not need these, do not compute... \n",
    "    # LH = jnp.linalg.cholesky(H_inv)\n",
    "    # laplace_approximation = npd.MultivariateNormal(f_hat.squeeze(), scale_tril = LH)\n",
    "\n",
    "    # Construct laplace distribution on test values. \n",
    "    map_latent_dist = posterior.predict(X_test, train_data = D)\n",
    "\n",
    "    Kxt = posterior.prior.kernel.cross_covariance(X_train, X_test)\n",
    "    Kxx = posterior.prior.kernel.gram(X_train)\n",
    "    Kxx += jnp.eye(D.n) * jitter \n",
    "    Kxx = PSD(Kxx)\n",
    "\n",
    "    Kxx_inv_Kxt = solve(Kxx, Kxt)\n",
    "    laplace_cov_term = jnp.matmul(jnp.matmul(Kxx_inv_Kxt.T, H_inv), Kxx_inv_Kxt)\n",
    "\n",
    "    mean = map_latent_dist.mean\n",
    "    covariance = map_latent_dist.covariance_matrix + laplace_cov_term\n",
    "    L = jnp.linalg.cholesky(covariance)\n",
    "    laplace_latent_dist = npd.MultivariateNormal(jnp.atleast_1d(mean.squeeze()), scale_tril = L)\n",
    "\n",
    "    return laplace_latent_dist\n",
    "\n",
    "def _compute_softmax(logits, beta):\n",
    "    \"\"\"\n",
    "    Compute softmax. \n",
    "\n",
    "    Parameters \n",
    "    ---------- \n",
    "    logits : \n",
    "    beta : temperature of the softmax\n",
    "\n",
    "    Returns \n",
    "    ----------\n",
    "    values : probabilities at each point after applying the softmax. \n",
    "    \"\"\"\n",
    "    return jax.nn.softmax(logits / beta) \n",
    "\n",
    "def _compute_ucb_softmax_sticky(mu, std, prev_choice, ucb, beta, stickyness):\n",
    "    \"\"\" \n",
    "    Compute UCB, add a sticky term and then apply softmax to convert into probabilities. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mu, std, prev_choice, ucb, beta, stickyness\n",
    "    \n",
    "    Returns\n",
    "    ---------- \n",
    "    probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    prob = (mu + ucb * std)*beta + prev_choice*stickyness \n",
    "    prob = prob - jnp.max(prob) # To deal with overflow errors apparently? \n",
    "    return jax.nn.softmax(prob)\n",
    "\n",
    "def _compute_ucb_softmax(mu, std, ucb, beta):\n",
    "    \"\"\"\n",
    "    Compute UCB and apply softmax to convert into probabilities.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mu, std, ucb, beta\n",
    "\n",
    "    Returns\n",
    "    ---------- \n",
    "    probabilities\n",
    "    \"\"\"\n",
    "    prob = (mu + ucb * std)*beta\n",
    "    prob = prob - jnp.max(prob) # To deal with overflow errors apparently? \n",
    "    return jax.nn.softmax(prob)\n",
    "\n",
    "def run_GP_single_trial(actions, rewards, hyperparams, key = jr.PRNGKey(42), jitter = 1e-6):\n",
    "\n",
    "    assert actions.shape == rewards.shape \n",
    "\n",
    "    ## Remove NaN trials and create dataset to work with. \n",
    "    mask = ~np.isnan(actions)\n",
    "    actions = np.expand_dims(actions[mask], 1)\n",
    "    rewards = np.expand_dims(rewards[mask], 1)    \n",
    "    D = gpx.Dataset(X = actions, y = rewards)\n",
    "    action_space = np.expand_dims(np.arange(1,5), 1)\n",
    "\n",
    "    _, posterior = _setup_GP(D.n, hyperparams)\n",
    "    print(\"setup GP.\")\n",
    "\n",
    "    ## Fit GP using Adam, get first approximation of MAP estimate \n",
    "    opt_posterior, _ = gpx.fit(\n",
    "        model = posterior,\n",
    "        objective = lambda p, d: -gpx.objectives.log_posterior_density(p, d),\n",
    "        train_data = D,\n",
    "        optim = ox.adamw(learning_rate=0.01),\n",
    "        num_iters=100,\n",
    "        key = key,\n",
    "        trainable=Parameter, \n",
    "        verbose = False,\n",
    "    )\n",
    "    print(\"optimized GP.\")\n",
    "\n",
    "    # If using softmax, uncomment these lines, do not need the Laplace approximation. \n",
    "    # map_latent_dist = opt_posterior.predict(angle_space, train_data = D)\n",
    "    # predictive_dist = opt_posterior.likelihood(map_latent_dist)\n",
    "    # predictive_mean = predictive_dist.mean\n",
    "    # probabilities = _compute_softmax(predictive_mean, hyperparams['beta'])\n",
    "\n",
    "    # Now compute Laplace approximation, and evaluate it on range, Do not need the Laplace approximation imo.\n",
    "    laplace_dist = _compute_laplace(opt_posterior, actions, action_space, D)\n",
    "    print(\"computed laplace.\")\n",
    "    predictive_dist = opt_posterior.likelihood(laplace_dist)\n",
    "    predictive_mean = predictive_dist.mean\n",
    "    predictive_std = jnp.sqrt(predictive_dist.variance)\n",
    "\n",
    "\n",
    "    prev_choices = np.zeros(shape = action_space.shape[0])\n",
    "    idx = (action_space.flatten() == actions[-1]) # find index for last angle\n",
    "    prev_choices[idx] = 1 \n",
    "    probabilities = _compute_ucb_softmax(predictive_mean, predictive_std, hyperparams['ucb'], hyperparams['beta'])\n",
    "    print(\"computed probabilities.\")\n",
    "    return probabilities\n",
    "\n",
    "def fit_GP_animal(actions, rewards, hyperparams):\n",
    "\n",
    "    nsessions, ntrials = actions.shape\n",
    "\n",
    "    action_space = np.arange(1,5)   #[1,2,3,4]\n",
    "    ll = np.zeros(shape = (nsessions, ntrials)) * np.nan \n",
    "\n",
    "    ## Parallelize this, otherwise its going to be insanely slow.     \n",
    "    for s in range(nsessions):\n",
    "        for t in range(1, ntrials):\n",
    "\n",
    "            if t == (ntrials - 1):\n",
    "                break \n",
    "            # Fit GP using trials upto t, and then evaluate probability of choosing angle t+1 \n",
    "            curr_actions = actions[s, :t]\n",
    "            curr_rewards = rewards[s, :t]\n",
    "\n",
    "            # Fit GP and get probabilities. \n",
    "            probabilities = run_GP_single_trial(curr_actions, curr_rewards, hyperparams)\n",
    "            log_probability = np.log(probabilities)\n",
    "            # idx = np.argwhere(np.round(actions[s, t], 2) == action_space)\n",
    "            idx = (action_space.flatten() == actions[s, t])\n",
    "            ll[s, t] = float(log_probability[idx])\n",
    "            print(f\"fit {t} trials.\")\n",
    "            \n",
    "    return ll\n",
    "\n",
    "def optimize_gp(dataset, rat): #(rat, datadir)\n",
    "\n",
    "    # bd = BehaviouralData()\n",
    "    actions = dataset[rat].port.to_numpy().reshape(-1, 100)[0:100]\n",
    "    rewards = dataset[rat].reward.to_numpy().reshape(-1, 100)[0:100]\n",
    "\n",
    "    def likelihood(x, actions = actions, rewards = rewards):\n",
    "\n",
    "        lengthscale, beta, ucb, stickyness = x\n",
    "        hyperparams = {'beta': beta, \n",
    "                       'ucb': ucb,\n",
    "                       'lengthscale': lengthscale,\n",
    "                        'stickyness':stickyness}\n",
    "        \n",
    "        ll = fit_GP_animal(actions, rewards, hyperparams)\n",
    "        return -np.nansum(ll)\n",
    "    \n",
    "    lb = np.array([0, 0, 0, 0])\n",
    "    ub = np.array([10, 10, 10, 1e-5])\n",
    "    plb = np.array([0.1, 0.1, 0.1, 0])\n",
    "    pub = np.array([1, 1, 1, 1e-5])\n",
    "    bads = BADS(likelihood, None, lb, ub, plb, pub, options = {'random_seed':42})\n",
    "    optresult = bads.optimize()\n",
    "    # with open(f'{datadir}/{rat+1}-gp.pkl', 'wb') as f:\n",
    "    #     pickle.dump(optresult, f)\n",
    "    return optresult\n",
    "\n",
    "\n",
    "# testing how long it takes for 1 session of 100 trials\n",
    "# train_data, test_data = load_data_into_dict()\n",
    "rat = 'test05022023'\n",
    "actions = train_data[rat].port.to_numpy().reshape(-1, 100)[:1, 0:100]\n",
    "rewards = train_data[rat].reward.to_numpy().reshape(-1, 100)[:1, 0:100]\n",
    "def likelihood(x, actions = actions, rewards = rewards):\n",
    "\n",
    "    lengthscale, beta, ucb, stickyness = x\n",
    "    hyperparams = {'beta': beta, \n",
    "                    'ucb': ucb,\n",
    "                    'lengthscale': lengthscale,\n",
    "                    'stickyness':stickyness}\n",
    "    \n",
    "    ll = fit_GP_animal(actions, rewards, hyperparams)\n",
    "    return -np.nansum(ll)\n",
    "\n",
    "x = [0.1, 0.1, 0.1, 0.1]\n",
    "nll = likelihood(x, actions, rewards)\n",
    "print(nll)\n",
    "\n",
    "# optresult = optimize_gp(train_data, rat)\n",
    "# print(optresult)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d01cfa",
   "metadata": {},
   "source": [
    "## GPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f93479d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " c:\\Users\\dlab\\rishika_sim\\utils\\dfLoading.py:60: SettingWithCopyWarning:\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      " c:\\Users\\dlab\\rishika_sim\\utils\\dfLoading.py:63: SettingWithCopyWarning:\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      " c:\\Users\\dlab\\rishika_sim\\utils\\dfLoading.py:65: SettingWithCopyWarning:\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      " c:\\Users\\dlab\\rishika_sim\\utils\\dfLoading.py:67: SettingWithCopyWarning:\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data\n"
     ]
    }
   ],
   "source": [
    "## GPy\n",
    "import GPy\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from scipy.optimize import minimize\n",
    "train_data, test_data = load_data_into_dict()\n",
    "\n",
    "def nLL_gp_ucb_gpy(x0, df, arms):\n",
    "    beta, tau, ls, lr_sig = x0\n",
    "    ll = 0\n",
    "    sessions = df.session.value_counts()\n",
    "    chosen_action = df.port.to_numpy()\n",
    "    rewarded = df.reward.to_numpy()\n",
    "    p = np.ones(len(df))*1e-7\n",
    "    session_ends = np.cumsum(sessions)\n",
    "    \n",
    "    sess_num = 0\n",
    "    sess_start = True\n",
    "    # lik = GPy.likelihoods.Bernoulli()\n",
    "\n",
    "    # gp settings\n",
    "    X = np.linspace(1,arms,arms).reshape(arms,1)\n",
    "    knl = GPy.kern.RBF(input_dim = 1, variance=lr_sig, lengthscale=ls)\n",
    "\n",
    "    for trial in range(len(chosen_action)):\n",
    "        # check if sess restarted\n",
    "        if sess_start == True:\n",
    "            # if group.block_group.unique()[0] == 1:\n",
    "            start_trial = trial\n",
    "\n",
    "        sess_start = False\n",
    "        \n",
    "        if len(np.unique(rewarded[start_trial:trial])) > 1:\n",
    "            # what actions were taken so far\n",
    "            a = chosen_action[start_trial:trial]\n",
    "\n",
    "            # what rewards were given for each action\n",
    "            r = rewarded[start_trial:trial]\n",
    "\n",
    "            # update gp using all the info we got so far on actions and rew\n",
    "            # try:\n",
    "            gp = GPy.models.GPClassification(a.reshape(-1, 1), r.reshape(-1, 1), kernel = knl, inference_method = GPy.inference.latent_function_inference.expectation_propagation.EP())\n",
    "            # except Exception as e:\n",
    "            #     print(a, r, sess_num, trial)\n",
    "            #     print(e)\n",
    "            #     break\n",
    "            mu, var = gp.predict_noiseless(X, kern = knl)\n",
    "            # print(mu, var)\n",
    "            mu_star = expit(mu)\n",
    "            sd_star = expit(mu+np.sqrt(var)) - expit(mu)\n",
    "\n",
    "            # calculate probability of taking any action\n",
    "            input_to_softmax = (mu_star + beta*sd_star)/tau\n",
    "            P = np.exp(input_to_softmax - max(input_to_softmax))\n",
    "            P = P/ np.sum(P)\n",
    "            P = np.clip(P, a_min = 1e-7, a_max = None)\n",
    "    \n",
    "            # what was probability of action taken?\n",
    "            chosen = int(chosen_action[trial]-1)\n",
    "            p[trial] = P[chosen]\n",
    "\n",
    "        # check how many trials elapsed\n",
    "        if trial == session_ends.iloc[sess_num]:\n",
    "            sess_start=True\n",
    "            sess_num += 1\n",
    "    p[p == 1e-7] = np.nan\n",
    "    ll += np.nansum(np.log(p))\n",
    "    nll = -ll\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71e3555c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7005.031712683343"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nLL_gp_ucb_gpy([0.1, 1, 1, 2], train_data['test05022023'], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe00b883",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m opt_gpy = \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnLL_gp_ucb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtest05022023\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dlab\\miniforge3\\envs\\gpy_env\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:779\u001b[39m, in \u001b[36mminimize\u001b[39m\u001b[34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[39m\n\u001b[32m    777\u001b[39m     res = _minimize_cg(fun, x0, args, jac, callback, **options)\n\u001b[32m    778\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m meth == \u001b[33m'\u001b[39m\u001b[33mbfgs\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m779\u001b[39m     res = \u001b[43m_minimize_bfgs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m meth == \u001b[33m'\u001b[39m\u001b[33mnewton-cg\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    781\u001b[39m     res = _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[32m    782\u001b[39m                              **options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dlab\\miniforge3\\envs\\gpy_env\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:1443\u001b[39m, in \u001b[36m_minimize_bfgs\u001b[39m\u001b[34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, finite_diff_rel_step, xrtol, c1, c2, hess_inv0, workers, **unknown_options)\u001b[39m\n\u001b[32m   1440\u001b[39m pk = -np.dot(Hk, gfk)\n\u001b[32m   1441\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1442\u001b[39m     alpha_k, fc, gc, old_fval, old_old_fval, gfkp1 = \\\n\u001b[32m-> \u001b[39m\u001b[32m1443\u001b[39m              \u001b[43m_line_search_wolfe12\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmyfprime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgfk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1444\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43mold_fval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mold_old_fval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamin\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1445\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43mamax\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mc1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mc2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1446\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _LineSearchError:\n\u001b[32m   1447\u001b[39m     \u001b[38;5;66;03m# Line search failed to find a better solution.\u001b[39;00m\n\u001b[32m   1448\u001b[39m     warnflag = \u001b[32m2\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dlab\\miniforge3\\envs\\gpy_env\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:1172\u001b[39m, in \u001b[36m_line_search_wolfe12\u001b[39m\u001b[34m(f, fprime, xk, pk, gfk, old_fval, old_old_fval, **kwargs)\u001b[39m\n\u001b[32m   1158\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1159\u001b[39m \u001b[33;03mSame as line_search_wolfe1, but fall back to line_search_wolfe2 if\u001b[39;00m\n\u001b[32m   1160\u001b[39m \u001b[33;03msuitable step length is not found, and raise an exception if a\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1167\u001b[39m \n\u001b[32m   1168\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1170\u001b[39m extra_condition = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33mextra_condition\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1172\u001b[39m ret = \u001b[43mline_search_wolfe1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfprime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgfk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mold_fval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mold_old_fval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m                         \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ret[\u001b[32m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m extra_condition \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1177\u001b[39m     xp1 = xk + ret[\u001b[32m0\u001b[39m] * pk\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dlab\\miniforge3\\envs\\gpy_env\\Lib\\site-packages\\scipy\\optimize\\_linesearch.py:93\u001b[39m, in \u001b[36mline_search_wolfe1\u001b[39m\u001b[34m(f, fprime, xk, pk, gfk, old_fval, old_old_fval, args, c1, c2, amax, amin, xtol)\u001b[39m\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.dot(gval[\u001b[32m0\u001b[39m], pk)\n\u001b[32m     91\u001b[39m derphi0 = np.dot(gfk, pk)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m stp, fval, old_fval = \u001b[43mscalar_search_wolfe1\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mphi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mderphi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mold_fval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mold_old_fval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mderphi0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43mc1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mc1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mc2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxtol\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxtol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m stp, fc[\u001b[32m0\u001b[39m], gc[\u001b[32m0\u001b[39m], fval, old_fval, gval[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dlab\\miniforge3\\envs\\gpy_env\\Lib\\site-packages\\scipy\\optimize\\_linesearch.py:170\u001b[39m, in \u001b[36mscalar_search_wolfe1\u001b[39m\u001b[34m(phi, derphi, phi0, old_phi0, derphi0, c1, c2, amax, amin, xtol)\u001b[39m\n\u001b[32m    167\u001b[39m maxiter = \u001b[32m100\u001b[39m\n\u001b[32m    169\u001b[39m dcsrch = DCSRCH(phi, derphi, c1, c2, xtol, amin, amax)\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m stp, phi1, phi0, task = \u001b[43mdcsrch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphi0\u001b[49m\u001b[43m=\u001b[49m\u001b[43mphi0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mderphi0\u001b[49m\u001b[43m=\u001b[49m\u001b[43mderphi0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaxiter\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m stp, phi1, phi0\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dlab\\miniforge3\\envs\\gpy_env\\Lib\\site-packages\\scipy\\optimize\\_dcsrch.py:256\u001b[39m, in \u001b[36mDCSRCH.__call__\u001b[39m\u001b[34m(self, alpha1, phi0, derphi0, maxiter)\u001b[39m\n\u001b[32m    254\u001b[39m     alpha1 = stp\n\u001b[32m    255\u001b[39m     phi1 = \u001b[38;5;28mself\u001b[39m.phi(stp)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     derphi1 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mderphi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    258\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dlab\\miniforge3\\envs\\gpy_env\\Lib\\site-packages\\scipy\\optimize\\_linesearch.py:87\u001b[39m, in \u001b[36mline_search_wolfe1.<locals>.derphi\u001b[39m\u001b[34m(s)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mderphi\u001b[39m(s):\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     gval[\u001b[32m0\u001b[39m] = \u001b[43mfprime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxk\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m     gc[\u001b[32m0\u001b[39m] += \u001b[32m1\u001b[39m\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.dot(gval[\u001b[32m0\u001b[39m], pk)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dlab\\miniforge3\\envs\\gpy_env\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:391\u001b[39m, in \u001b[36mScalarFunction.grad\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.array_equal(x, \u001b[38;5;28mself\u001b[39m.x):\n\u001b[32m    390\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_x(x)\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.g\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dlab\\miniforge3\\envs\\gpy_env\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:366\u001b[39m, in \u001b[36mScalarFunction._update_grad\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._orig_grad \u001b[38;5;129;01min\u001b[39;00m FD_METHODS:\n\u001b[32m    365\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_fun()\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m \u001b[38;5;28mself\u001b[39m.g = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wrapped_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[38;5;28mself\u001b[39m.g_updated = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dlab\\miniforge3\\envs\\gpy_env\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:41\u001b[39m, in \u001b[36m_ScalarGradWrapper.__call__\u001b[39m\u001b[34m(self, x, f0, **kwds)\u001b[39m\n\u001b[32m     39\u001b[39m     g = np.atleast_1d(\u001b[38;5;28mself\u001b[39m.grad(np.copy(x), *\u001b[38;5;28mself\u001b[39m.args))\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.grad \u001b[38;5;129;01min\u001b[39;00m FD_METHODS:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     g, dct = \u001b[43mapprox_derivative\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m        \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m=\u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfinite_diff_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28mself\u001b[39m.nfev += dct[\u001b[33m'\u001b[39m\u001b[33mnfev\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     49\u001b[39m \u001b[38;5;28mself\u001b[39m.ngev += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dlab\\miniforge3\\envs\\gpy_env\\Lib\\site-packages\\scipy\\optimize\\_numdiff.py:593\u001b[39m, in \u001b[36mapprox_derivative\u001b[39m\u001b[34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs, full_output, workers)\u001b[39m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m MapWrapper(workers) \u001b[38;5;28;01mas\u001b[39;00m mf:\n\u001b[32m    592\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sparsity \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m         J, _nfev = \u001b[43m_dense_difference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43muse_one_sided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mmf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    596\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    597\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(sparsity) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sparsity) == \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dlab\\miniforge3\\envs\\gpy_env\\Lib\\site-packages\\scipy\\optimize\\_numdiff.py:686\u001b[39m, in \u001b[36m_dense_difference\u001b[39m\u001b[34m(fun, x0, f0, h, use_one_sided, method, workers)\u001b[39m\n\u001b[32m    684\u001b[39m f_evals = workers(fun, x_generator2(x0, h))\n\u001b[32m    685\u001b[39m dx = [(x0[i] + h[i]) - x0[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n)]\n\u001b[32m--> \u001b[39m\u001b[32m686\u001b[39m df = \u001b[43m[\u001b[49m\u001b[43mf_eval\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf_eval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf_evals\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    687\u001b[39m df_dx = [delf / delx \u001b[38;5;28;01mfor\u001b[39;00m delf, delx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(df, dx)]\n\u001b[32m    688\u001b[39m nfev += \u001b[38;5;28mlen\u001b[39m(df_dx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dlab\\miniforge3\\envs\\gpy_env\\Lib\\site-packages\\scipy\\optimize\\_numdiff.py:686\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    684\u001b[39m f_evals = workers(fun, x_generator2(x0, h))\n\u001b[32m    685\u001b[39m dx = [(x0[i] + h[i]) - x0[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n)]\n\u001b[32m--> \u001b[39m\u001b[32m686\u001b[39m df = \u001b[43m[\u001b[49m\u001b[43mf_eval\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf_eval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf_evals\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    687\u001b[39m df_dx = [delf / delx \u001b[38;5;28;01mfor\u001b[39;00m delf, delx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(df, dx)]\n\u001b[32m    688\u001b[39m nfev += \u001b[38;5;28mlen\u001b[39m(df_dx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dlab\\miniforge3\\envs\\gpy_env\\Lib\\site-packages\\scipy\\optimize\\_numdiff.py:879\u001b[39m, in \u001b[36m_Fun_Wrapper.__call__\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    876\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m xp.isdtype(x.dtype, \u001b[33m\"\u001b[39m\u001b[33mreal floating\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    877\u001b[39m     x = xp.astype(x, \u001b[38;5;28mself\u001b[39m.x0.dtype)\n\u001b[32m--> \u001b[39m\u001b[32m879\u001b[39m f = np.atleast_1d(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    880\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m f.ndim > \u001b[32m1\u001b[39m:\n\u001b[32m    881\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m`fun` return value has \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    882\u001b[39m                        \u001b[33m\"\u001b[39m\u001b[33mmore than 1 dimension.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dlab\\miniforge3\\envs\\gpy_env\\Lib\\site-packages\\scipy\\_lib\\_util.py:590\u001b[39m, in \u001b[36m_ScalarFunctionWrapper.__call__\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    587\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    588\u001b[39m     \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[32m    589\u001b[39m     \u001b[38;5;66;03m# The user of this class might want `x` to remain unchanged.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m590\u001b[39m     fx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m     \u001b[38;5;28mself\u001b[39m.nfev += \u001b[32m1\u001b[39m\n\u001b[32m    593\u001b[39m     \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mnLL_gp_ucb\u001b[39m\u001b[34m(x0, df, arms)\u001b[39m\n\u001b[32m     38\u001b[39m r = rewarded[start_trial:trial]\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# update gp using all the info we got so far on actions and rew\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m gp = \u001b[43mGPy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGPClassification\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mknl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minference_method\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mGPy\u001b[49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlatent_function_inference\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpectation_propagation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEP\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# except Exception as e:\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m#     print(a, r, sess_num, trial)\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m#     print(e)\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m#     break\u001b[39;00m\n\u001b[32m     47\u001b[39m mu, var = gp.predict_noiseless(X, kern = knl)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dlab\\miniforge3\\envs\\gpy_env\\Lib\\site-packages\\paramz\\parameterized.py:58\u001b[39m, in \u001b[36mParametersChangedMeta.__call__\u001b[39m\u001b[34m(self, *args, **kw)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28mself\u001b[39m._model_initialized_ = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m initialize:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minitialize_parameter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dlab\\miniforge3\\envs\\gpy_env\\Lib\\site-packages\\paramz\\core\\parameter_core.py:337\u001b[39m, in \u001b[36mParameterizable.initialize_parameter\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    335\u001b[39m \u001b[38;5;28mself\u001b[39m._highest_parent_._connect_parameters() \u001b[38;5;66;03m#logger.debug(\"calling parameters changed\")\u001b[39;00m\n\u001b[32m    336\u001b[39m \u001b[38;5;28mself\u001b[39m._highest_parent_._connect_fixes()\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrigger_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dlab\\miniforge3\\envs\\gpy_env\\Lib\\site-packages\\paramz\\core\\updateable.py:79\u001b[39m, in \u001b[36mUpdateable.trigger_update\u001b[39m\u001b[34m(self, trigger_parent)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.update_model() \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_in_init_\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._in_init_):\n\u001b[32m     77\u001b[39m     \u001b[38;5;66;03m#print \"Warning: updates are off, updating the model will do nothing\"\u001b[39;00m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_trigger_params_changed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrigger_parent\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dlab\\miniforge3\\envs\\gpy_env\\Lib\\site-packages\\paramz\\core\\parameter_core.py:134\u001b[39m, in \u001b[36mOptimizationHandlable._trigger_params_changed\u001b[39m\u001b[34m(self, trigger_parent)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[33;03mFirst tell all children to update,\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03mthen update yourself.\u001b[39;00m\n\u001b[32m    130\u001b[39m \n\u001b[32m    131\u001b[39m \u001b[33;03mIf trigger_parent is True, we will tell the parent, otherwise not.\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    133\u001b[39m [p._trigger_params_changed(trigger_parent=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parameters \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m p.is_fixed]\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnotify_observers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrigger_parent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43minf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dlab\\miniforge3\\envs\\gpy_env\\Lib\\site-packages\\paramz\\core\\observable.py:91\u001b[39m, in \u001b[36mObservable.notify_observers\u001b[39m\u001b[34m(self, which, min_priority)\u001b[39m\n\u001b[32m     89\u001b[39m     which = \u001b[38;5;28mself\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m min_priority \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     \u001b[43m[\u001b[49m\u001b[43mcallble\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhich\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhich\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallble\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobservers\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m p, _, callble \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.observers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dlab\\miniforge3\\envs\\gpy_env\\Lib\\site-packages\\paramz\\core\\observable.py:91\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     89\u001b[39m     which = \u001b[38;5;28mself\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m min_priority \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     [\u001b[43mcallble\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhich\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhich\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _, _, callble \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.observers]\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m p, _, callble \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.observers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dlab\\miniforge3\\envs\\gpy_env\\Lib\\site-packages\\paramz\\core\\parameter_core.py:508\u001b[39m, in \u001b[36mParameterizable._parameters_changed_notification\u001b[39m\u001b[34m(self, me, which)\u001b[39m\n\u001b[32m    503\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    504\u001b[39m \u001b[33;03mIn parameterizable we just need to make sure, that the next call to optimizer_array\u001b[39;00m\n\u001b[32m    505\u001b[39m \u001b[33;03mwill update the optimizer_array to the latest parameters\u001b[39;00m\n\u001b[32m    506\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    507\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_copy_transformed = \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;66;03m# tells the optimizer array to update on next request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparameters_changed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dlab\\miniforge3\\envs\\gpy_env\\Lib\\site-packages\\GPy\\core\\gp.py:278\u001b[39m, in \u001b[36mGP.parameters_changed\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparameters_changed\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    270\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    271\u001b[39m \u001b[33;03m    Method that is called upon any changes to :class:`~GPy.core.parameterization.param.Param` variables within the model.\u001b[39;00m\n\u001b[32m    272\u001b[39m \u001b[33;03m    In particular in the GP class this method re-performs inference, recalculating the posterior and log marginal likelihood and gradients of the model\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    276\u001b[39m \u001b[33;03m        this method yourself, there may be unexpected consequences.\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m     \u001b[38;5;28mself\u001b[39m.posterior, \u001b[38;5;28mself\u001b[39m._log_marginal_likelihood, \u001b[38;5;28mself\u001b[39m.grad_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minference_method\u001b[49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlikelihood\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mY_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmean_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mY_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m     \u001b[38;5;28mself\u001b[39m.likelihood.update_gradients(\u001b[38;5;28mself\u001b[39m.grad_dict[\u001b[33m'\u001b[39m\u001b[33mdL_dthetaL\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    280\u001b[39m     \u001b[38;5;28mself\u001b[39m.kern.update_gradients_full(\u001b[38;5;28mself\u001b[39m.grad_dict[\u001b[33m'\u001b[39m\u001b[33mdL_dK\u001b[39m\u001b[33m'\u001b[39m], \u001b[38;5;28mself\u001b[39m.X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dlab\\miniforge3\\envs\\gpy_env\\Lib\\site-packages\\GPy\\inference\\latent_function_inference\\expectation_propagation.py:275\u001b[39m, in \u001b[36mEP.inference\u001b[39m\u001b[34m(self, kern, X, likelihood, Y, mean_function, Y_metadata, precision, K)\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ep_mode==\u001b[33m\"\u001b[39m\u001b[33malternated\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loading:\n\u001b[32m    273\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m_ep_approximation\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    274\u001b[39m         \u001b[38;5;66;03m#if we don't yet have the results of runnign EP, run EP and store the computed factors in self._ep_approximation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m         post_params, ga_approx, cav_params, log_Z_tilde = \u001b[38;5;28mself\u001b[39m._ep_approximation = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexpectation_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean_prior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlikelihood\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    277\u001b[39m         \u001b[38;5;66;03m#if we've already run EP, just use the existing approximation stored in self._ep_approximation\u001b[39;00m\n\u001b[32m    278\u001b[39m         post_params, ga_approx, cav_params, log_Z_tilde = \u001b[38;5;28mself\u001b[39m._ep_approximation\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dlab\\miniforge3\\envs\\gpy_env\\Lib\\site-packages\\GPy\\inference\\latent_function_inference\\expectation_propagation.py:304\u001b[39m, in \u001b[36mEP.expectation_propagation\u001b[39m\u001b[34m(self, mean_prior, K, Y, likelihood, Y_metadata)\u001b[39m\n\u001b[32m    302\u001b[39m iterations = \u001b[32m0\u001b[39m\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stop \u001b[38;5;129;01mand\u001b[39;00m (iterations < \u001b[38;5;28mself\u001b[39m.max_iters):\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_local_updates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcav_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmarg_moments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mga_approx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlikelihood\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    306\u001b[39m     \u001b[38;5;66;03m#(re) compute Sigma and mu using full Cholesky decompy\u001b[39;00m\n\u001b[32m    307\u001b[39m     post_params = posteriorParams._recompute(mean_prior, K, ga_approx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dlab\\miniforge3\\envs\\gpy_env\\Lib\\site-packages\\GPy\\inference\\latent_function_inference\\expectation_propagation.py:358\u001b[39m, in \u001b[36mEP._local_updates\u001b[39m\u001b[34m(self, num_data, cav_params, post_params, marg_moments, ga_approx, likelihood, Y, Y_metadata, update_order)\u001b[39m\n\u001b[32m    355\u001b[39m delta_tau, delta_v = ga_approx._update_i(\u001b[38;5;28mself\u001b[39m.eta, \u001b[38;5;28mself\u001b[39m.delta, post_params, marg_moments, i)\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parallel_updates == \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m358\u001b[39m     \u001b[43mpost_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_update_rank1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta_tau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mga_approx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dlab\\miniforge3\\envs\\gpy_env\\Lib\\site-packages\\GPy\\inference\\latent_function_inference\\expectation_propagation.py:105\u001b[39m, in \u001b[36mposteriorParams._update_rank1\u001b[39m\u001b[34m(self, delta_tau, delta_v, ga_approx, i)\u001b[39m\n\u001b[32m    103\u001b[39m ci = delta_tau/(\u001b[32m1.\u001b[39m+ delta_tau*si[i])\n\u001b[32m    104\u001b[39m \u001b[38;5;28mself\u001b[39m.mu = \u001b[38;5;28mself\u001b[39m.mu - (ci*(\u001b[38;5;28mself\u001b[39m.mu[i]+si[i]*delta_v)-delta_v) * si\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m \u001b[43mDSYR\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mSigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43mci\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dlab\\miniforge3\\envs\\gpy_env\\Lib\\site-packages\\GPy\\util\\linalg.py:353\u001b[39m, in \u001b[36mDSYR\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mDSYR\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDSYR_blas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dlab\\miniforge3\\envs\\gpy_env\\Lib\\site-packages\\GPy\\util\\linalg.py:335\u001b[39m, in \u001b[36mDSYR_blas\u001b[39m\u001b[34m(A, x, alpha)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mDSYR_blas\u001b[39m(A, x, alpha=\u001b[32m1.\u001b[39m):\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[33;03m    Performs a symmetric rank-1 update operation:\u001b[39;00m\n\u001b[32m    328\u001b[39m \u001b[33;03m    A <- A + alpha * np.dot(x,x.T)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    333\u001b[39m \n\u001b[32m    334\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     At = \u001b[43mblas\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdsyr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m=\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_a\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#See https://github.com/scipy/scipy/issues/8155\u001b[39;00m\n\u001b[32m    336\u001b[39m     A[:] = At\n\u001b[32m    337\u001b[39m     symmetrify(A, upper=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "opt_gpy = minimize(nLL_gp_ucb_gpy, [0.1, 1, 1, 2], args = (train_data['test05022023'], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a395dec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.07635476]\n",
      " [-0.34219828]\n",
      " [-0.56418958]\n",
      " [-0.34219828]] [[0.99416995]\n",
      " [0.88290034]\n",
      " [0.68169011]\n",
      " [0.88290034]]\n",
      "[[-0.16778838]\n",
      " [-0.75197537]\n",
      " [-1.23979778]\n",
      " [-0.75197537]] [[2.32067894]\n",
      " [1.7833705 ]\n",
      " [0.81174897]\n",
      " [1.7833705 ]]\n",
      "[[-1.94384472]\n",
      " [-1.94389576]\n",
      " [-1.94388582]\n",
      " [-1.94381489]] [[0.93576123]\n",
      " [0.9355847 ]\n",
      " [0.93564517]\n",
      " [0.93594265]]\n",
      "[[-2.58604409]\n",
      " [-2.58612522]\n",
      " [-2.58612521]\n",
      " [-2.58604407]] [[1.52563956]\n",
      " [1.52522005]\n",
      " [1.52522006]\n",
      " [1.52563957]]\n",
      "[[-6.09212810e-06]\n",
      " [-6.09242191e-06]\n",
      " [-6.09252458e-06]\n",
      " [-6.09243611e-06]] [[2.7938513e-06]\n",
      " [2.7938513e-06]\n",
      " [2.7938513e-06]\n",
      " [2.7938513e-06]]\n",
      "[[-8.91609086e-06]\n",
      " [-8.91651047e-06]\n",
      " [-8.91665035e-06]\n",
      " [-8.91651047e-06]] [[2.79383992e-06]\n",
      " [2.79383992e-06]\n",
      " [2.79383992e-06]\n",
      " [2.79383992e-06]]\n",
      "[[-0.64451207]\n",
      " [-0.64457212]\n",
      " [-0.64461195]\n",
      " [-0.64463155]] [[0.16160254]\n",
      " [0.16157833]\n",
      " [0.16158019]\n",
      " [0.16160812]]\n",
      "[[-0.93792732]\n",
      " [-0.93805053]\n",
      " [-0.93814431]\n",
      " [-0.93820865]] [[0.21469995]\n",
      " [0.21465229]\n",
      " [0.21465984]\n",
      " [0.21472262]]\n",
      "[[-1.05986202]\n",
      " [-1.06002103]\n",
      " [-1.06014678]\n",
      " [-1.06023926]] [[0.22504563]\n",
      " [0.22498395]\n",
      " [0.22499279]\n",
      " [0.22507215]]\n",
      "[[-0.64747892]\n",
      " [-0.64758298]\n",
      " [-0.64766672]\n",
      " [-0.64773014]] [[0.14771873]\n",
      " [0.14769985]\n",
      " [0.1477073 ]\n",
      " [0.14774108]]\n",
      "[[-0.36282634]\n",
      " [-0.36287958]\n",
      " [-0.36292144]\n",
      " [-0.36295192]] [[0.09338943]\n",
      " [0.09338409]\n",
      " [0.09338703]\n",
      " [0.09339823]]\n",
      "[[-0.46591551]\n",
      " [-0.46599366]\n",
      " [-0.46605719]\n",
      " [-0.46610609]] [[0.09910714]\n",
      " [0.09909719]\n",
      " [0.09910088]\n",
      " [0.09911819]]\n",
      "[[-0.5596556 ]\n",
      " [-0.55976276]\n",
      " [-0.55985236]\n",
      " [-0.55992439]] [[0.10386084]\n",
      " [0.10384598]\n",
      " [0.10385079]\n",
      " [0.10387528]]\n",
      "[[-0.63893799]\n",
      " [-0.63907467]\n",
      " [-0.63919129]\n",
      " [-0.63928785]] [[0.10583309]\n",
      " [0.10581313]\n",
      " [0.10581883]\n",
      " [0.10585018]]\n",
      "[[-0.70733748]\n",
      " [-0.7075037 ]\n",
      " [-0.70764772]\n",
      " [-0.70776953]] [[0.1062325 ]\n",
      " [0.10620738]\n",
      " [0.10621371]\n",
      " [0.10625148]]\n",
      "[[-0.52352335]\n",
      " [-0.52364204]\n",
      " [-0.52374431]\n",
      " [-0.52383014]] [[0.08633309]\n",
      " [0.08632072]\n",
      " [0.08632557]\n",
      " [0.08634765]]\n",
      "[[-0.58538517]\n",
      " [-0.58553019]\n",
      " [-0.58565684]\n",
      " [-0.5857651 ]] [[0.08557282]\n",
      " [0.08555636]\n",
      " [0.08556145]\n",
      " [0.08558807]]\n",
      "[[-0.64297986]\n",
      " [-0.64315338]\n",
      " [-0.64330672]\n",
      " [-0.64343987]] [[0.08584095]\n",
      " [0.08582061]\n",
      " [0.08582627]\n",
      " [0.08585792]]\n",
      "[[-0.49860724]\n",
      " [-0.49873595]\n",
      " [-0.49884902]\n",
      " [-0.49894643]] [[0.07267168]\n",
      " [0.07266046]\n",
      " [0.07266488]\n",
      " [0.07268493]]\n",
      "[[-0.55167407]\n",
      " [-0.55181694]\n",
      " [-0.5519425 ]\n",
      " [-0.55205073]] [[0.07226704]\n",
      " [0.07225431]\n",
      " [0.07226071]\n",
      " [0.07228625]]\n",
      "[[-0.60133444]\n",
      " [-0.60149748]\n",
      " [-0.60164164]\n",
      " [-0.60176691]] [[0.0724618 ]\n",
      " [0.07244717]\n",
      " [0.07245527]\n",
      " [0.07248612]]\n",
      "[[-0.48255652]\n",
      " [-0.48268127]\n",
      " [-0.48279088]\n",
      " [-0.48288534]] [[0.06308217]\n",
      " [0.06307346]\n",
      " [0.0630794 ]\n",
      " [0.06309998]]\n",
      "[[-0.37535834]\n",
      " [-0.37544856]\n",
      " [-0.37552699]\n",
      " [-0.37559364]] [[0.05355719]\n",
      " [0.05355219]\n",
      " [0.05355605]\n",
      " [0.05356877]]\n",
      "[[-0.4245269 ]\n",
      " [-0.42463081]\n",
      " [-0.42472141]\n",
      " [-0.42479867]] [[0.0544565 ]\n",
      " [0.05445053]\n",
      " [0.05445589]\n",
      " [0.05447259]]\n",
      "[[-0.47049895]\n",
      " [-0.47061633]\n",
      " [-0.47071893]\n",
      " [-0.47080677]] [[0.05531461]\n",
      " [0.05530797]\n",
      " [0.05531526]\n",
      " [0.05533645]]\n",
      "[[-0.37928873]\n",
      " [-0.37937748]\n",
      " [-0.37945433]\n",
      " [-0.37951927]] [[0.04872447]\n",
      " [0.04872047]\n",
      " [0.04872552]\n",
      " [0.04873961]]\n",
      "[[-0.29332997]\n",
      " [-0.29339309]\n",
      " [-0.293447  ]\n",
      " [-0.29349171]] [[0.04141676]\n",
      " [0.04141448]\n",
      " [0.04141761]\n",
      " [0.04142615]]\n",
      "[[-0.21097227]\n",
      " [-0.21101334]\n",
      " [-0.21104779]\n",
      " [-0.21107562]] [[0.0332584 ]\n",
      " [0.03325728]\n",
      " [0.03325895]\n",
      " [0.03326341]]\n",
      "[[-0.12940404]\n",
      " [-0.12942641]\n",
      " [-0.12944472]\n",
      " [-0.12945897]] [[0.02320128]\n",
      " [0.02320087]\n",
      " [0.02320152]\n",
      " [0.02320322]]\n",
      "[[-0.04511842]\n",
      " [-0.04512519]\n",
      " [-0.04513055]\n",
      " [-0.0451345 ]] [[0.00941516]\n",
      " [0.00941512]\n",
      " [0.0094152 ]\n",
      " [0.0094154 ]]\n",
      "[[-1.22191964e-05]\n",
      " [-1.22210753e-05]\n",
      " [-1.22225708e-05]\n",
      " [-1.22236828e-05]] [[3.06013246e-06]\n",
      " [3.06013246e-06]\n",
      " [3.06013247e-06]\n",
      " [3.06013249e-06]]\n",
      "[[-3.59229758e-06]\n",
      " [-3.59300215e-06]\n",
      " [-3.59359399e-06]\n",
      " [-3.59407305e-06]] [[1.12580287e-06]\n",
      " [1.12580287e-06]\n",
      " [1.12580287e-06]\n",
      " [1.12580288e-06]]\n",
      "[[-2.69403384e-06]\n",
      " [-2.69475249e-06]\n",
      " [-2.69538661e-06]\n",
      " [-2.69593611e-06]] [[1.12580207e-06]\n",
      " [1.12580207e-06]\n",
      " [1.12580207e-06]\n",
      " [1.12580207e-06]]\n",
      "[[-6.60638133e-07]\n",
      " [-6.60907697e-07]\n",
      " [-6.61156525e-07]\n",
      " [-6.61384595e-07]] [[4.14165665e-07]\n",
      " [4.14165665e-07]\n",
      " [4.14165665e-07]\n",
      " [4.14165665e-07]]\n",
      "[[-9.91094215e-07]\n",
      " [-9.91358595e-07]\n",
      " [-9.91591872e-07]\n",
      " [-9.91794024e-07]] [[4.14165555e-07]\n",
      " [4.14165555e-07]\n",
      " [4.14165556e-07]\n",
      " [4.14165556e-07]]\n",
      "[[-6.60637720e-07]\n",
      " [-6.60907284e-07]\n",
      " [-6.61156113e-07]\n",
      " [-6.61384182e-07]] [[4.14165446e-07]\n",
      " [4.14165446e-07]\n",
      " [4.14165446e-07]\n",
      " [4.14165447e-07]]\n",
      "[[-3.30181399e-07]\n",
      " [-3.30456147e-07]\n",
      " [-3.30720527e-07]\n",
      " [-3.30974514e-07]] [[4.14165337e-07]\n",
      " [4.14165337e-07]\n",
      " [4.14165337e-07]\n",
      " [4.14165338e-07]]\n",
      "[[ 2.74747195e-10]\n",
      " [-5.18480331e-12]\n",
      " [-2.85116314e-10]\n",
      " [-5.65020989e-10]] [[4.14165228e-07]\n",
      " [4.14165228e-07]\n",
      " [4.14165228e-07]\n",
      " [4.14165228e-07]]\n",
      "[[3.30730720e-07]\n",
      " [3.30445604e-07]\n",
      " [3.30150120e-07]\n",
      " [3.29844298e-07]] [[4.14165119e-07]\n",
      " [4.14165119e-07]\n",
      " [4.14165119e-07]\n",
      " [4.14165119e-07]]\n",
      "[[6.61186518e-07]\n",
      " [6.60896218e-07]\n",
      " [6.60585183e-07]\n",
      " [6.60253443e-07]] [[4.14165009e-07]\n",
      " [4.14165009e-07]\n",
      " [4.14165010e-07]\n",
      " [4.14165010e-07]]\n",
      "[[9.91642142e-07]\n",
      " [9.91346657e-07]\n",
      " [9.91020071e-07]\n",
      " [9.90662414e-07]] [[4.14164900e-07]\n",
      " [4.14164900e-07]\n",
      " [4.14164900e-07]\n",
      " [4.14164901e-07]]\n",
      "[[1.32209759e-06]\n",
      " [1.32179692e-06]\n",
      " [1.32145479e-06]\n",
      " [1.32107121e-06]] [[4.14164791e-07]\n",
      " [4.14164791e-07]\n",
      " [4.14164791e-07]\n",
      " [4.14164792e-07]]\n",
      "[[1.65255287e-06]\n",
      " [1.65224701e-06]\n",
      " [1.65188932e-06]\n",
      " [1.65147983e-06]] [[4.14164682e-07]\n",
      " [4.14164682e-07]\n",
      " [4.14164682e-07]\n",
      " [4.14164683e-07]]\n",
      "[[1.32209689e-06]\n",
      " [1.32179623e-06]\n",
      " [1.32145409e-06]\n",
      " [1.32107051e-06]] [[4.14164573e-07]\n",
      " [4.14164573e-07]\n",
      " [4.14164573e-07]\n",
      " [4.14164573e-07]]\n",
      "[[1.65255200e-06]\n",
      " [1.65224614e-06]\n",
      " [1.65188845e-06]\n",
      " [1.65147896e-06]] [[4.14164463e-07]\n",
      " [4.14164463e-07]\n",
      " [4.14164464e-07]\n",
      " [4.14164464e-07]]\n",
      "[[1.98300692e-06]\n",
      " [1.98269589e-06]\n",
      " [1.98232264e-06]\n",
      " [1.98188723e-06]] [[4.14164354e-07]\n",
      " [4.14164354e-07]\n",
      " [4.14164354e-07]\n",
      " [4.14164355e-07]]\n",
      "[[2.31346167e-06]\n",
      " [2.31314545e-06]\n",
      " [2.31275666e-06]\n",
      " [2.31229533e-06]] [[4.14164245e-07]\n",
      " [4.14164245e-07]\n",
      " [4.14164245e-07]\n",
      " [4.14164246e-07]]\n",
      "[[2.64391625e-06]\n",
      " [2.64359485e-06]\n",
      " [2.64319050e-06]\n",
      " [2.64270326e-06]] [[4.14164136e-07]\n",
      " [4.14164136e-07]\n",
      " [4.14164136e-07]\n",
      " [4.14164137e-07]]\n",
      "[[2.97437066e-06]\n",
      " [2.97404407e-06]\n",
      " [2.97362417e-06]\n",
      " [2.97311101e-06]] [[4.14164027e-07]\n",
      " [4.14164027e-07]\n",
      " [4.14164027e-07]\n",
      " [4.14164028e-07]]\n",
      "[[3.30482489e-06]\n",
      " [3.30449311e-06]\n",
      " [3.30405767e-06]\n",
      " [3.30351859e-06]] [[4.14163917e-07]\n",
      " [4.14163917e-07]\n",
      " [4.14163918e-07]\n",
      " [4.14163918e-07]]\n",
      "[[3.63527894e-06]\n",
      " [3.63494199e-06]\n",
      " [3.63449099e-06]\n",
      " [3.63392599e-06]] [[4.14163808e-07]\n",
      " [4.14163808e-07]\n",
      " [4.14163809e-07]\n",
      " [4.14163809e-07]]\n",
      "[[0.18501028]\n",
      " [0.1849658 ]\n",
      " [0.18491553]\n",
      " [0.18485945]] [[0.0193204 ]\n",
      " [0.01932036]\n",
      " [0.01932247]\n",
      " [0.01932673]]\n",
      "[[0.21641803]\n",
      " [0.21636045]\n",
      " [0.21629608]\n",
      " [0.21622493]] [[0.02095303]\n",
      " [0.02095287]\n",
      " [0.02095565]\n",
      " [0.02096138]]\n",
      "[[0.2418958 ]\n",
      " [0.2418266 ]\n",
      " [0.24174982]\n",
      " [0.24166545]] [[0.0217966 ]\n",
      " [0.02179641]\n",
      " [0.0217999 ]\n",
      " [0.02180707]]\n",
      "[[0.26583766]\n",
      " [0.26575635]\n",
      " [0.26566671]\n",
      " [0.26556874]] [[0.02240798]\n",
      " [0.02240777]\n",
      " [0.02241201]\n",
      " [0.02242069]]\n",
      "[[0.28848278]\n",
      " [0.28838891]\n",
      " [0.28828599]\n",
      " [0.28817403]] [[0.02285194]\n",
      " [0.02285171]\n",
      " [0.02285672]\n",
      " [0.02286696]]\n",
      "[[0.25329258]\n",
      " [0.25321836]\n",
      " [0.2531362 ]\n",
      " [0.2530461 ]] [[0.02137396]\n",
      " [0.02137384]\n",
      " [0.02137776]\n",
      " [0.02138571]]\n",
      "[[0.27524511]\n",
      " [0.27515942]\n",
      " [0.2750651 ]\n",
      " [0.27496215]] [[0.02177695]\n",
      " [0.02177685]\n",
      " [0.02178151]\n",
      " [0.02179094]]\n",
      "[[0.24142301]\n",
      " [0.2413552 ]\n",
      " [0.24127982]\n",
      " [0.24119687]] [[0.02035114]\n",
      " [0.0203511 ]\n",
      " [0.02035473]\n",
      " [0.02036203]]\n",
      "[[0.20790675]\n",
      " [0.20785062]\n",
      " [0.20778797]\n",
      " [0.20771881]] [[0.01872637]\n",
      " [0.01872628]\n",
      " [0.01872892]\n",
      " [0.01873427]]\n",
      "[[0.17457719]\n",
      " [0.1745323 ]\n",
      " [0.17448192]\n",
      " [0.17442608]] [[0.01689564]\n",
      " [0.01689555]\n",
      " [0.01689737]\n",
      " [0.0169011 ]]\n",
      "[[0.14112117]\n",
      " [0.14108691]\n",
      " [0.14104823]\n",
      " [0.14100513]] [[0.01476917]\n",
      " [0.01476909]\n",
      " [0.01477025]\n",
      " [0.01477267]]\n",
      "[[0.10714806]\n",
      " [0.10712375]\n",
      " [0.10709609]\n",
      " [0.10706507]] [[0.01221639]\n",
      " [0.01221633]\n",
      " [0.01221699]\n",
      " [0.01221837]]\n",
      "[[0.07213351]\n",
      " [0.07211839]\n",
      " [0.07210101]\n",
      " [0.07208137]] [[0.0090383 ]\n",
      " [0.00903826]\n",
      " [0.00903856]\n",
      " [0.00903918]]\n",
      "[[0.0353632 ]\n",
      " [0.03535676]\n",
      " [0.03534921]\n",
      " [0.03534055]] [[0.00492109]\n",
      " [0.00492108]\n",
      " [0.00492115]\n",
      " [0.0049213 ]]\n",
      "[[8.05273848e-05]\n",
      " [8.05137988e-05]\n",
      " [8.04976866e-05]\n",
      " [8.04790497e-05]] [[1.26072616e-05]\n",
      " [1.26072616e-05]\n",
      " [1.26072620e-05]\n",
      " [1.26072628e-05]]\n",
      "[[6.93490677e-06]\n",
      " [6.93352370e-06]\n",
      " [6.93192309e-06]\n",
      " [6.93010508e-06]] [[1.24143202e-06]\n",
      " [1.24143202e-06]\n",
      " [1.24143203e-06]\n",
      " [1.24143203e-06]]\n",
      "[[2.18691014e-06]\n",
      " [2.18638419e-06]\n",
      " [2.18578964e-06]\n",
      " [2.18512654e-06]] [[4.56712652e-07]\n",
      " [4.56712652e-07]\n",
      " [4.56712652e-07]\n",
      " [4.56712653e-07]]\n",
      "[[1.82255683e-06]\n",
      " [1.82200230e-06]\n",
      " [1.82139059e-06]\n",
      " [1.82072178e-06]] [[4.56712519e-07]\n",
      " [4.56712519e-07]\n",
      " [4.56712519e-07]\n",
      " [4.56712520e-07]]\n",
      "[[1.45820397e-06]\n",
      " [1.45762085e-06]\n",
      " [1.45699200e-06]\n",
      " [1.45631747e-06]] [[4.56712386e-07]\n",
      " [4.56712386e-07]\n",
      " [4.56712387e-07]\n",
      " [4.56712388e-07]]\n",
      "[[1.09385131e-06]\n",
      " [1.09323961e-06]\n",
      " [1.09259361e-06]\n",
      " [1.09191336e-06]] [[4.56712253e-07]\n",
      " [4.56712253e-07]\n",
      " [4.56712254e-07]\n",
      " [4.56712255e-07]]\n",
      "[[7.29498874e-07]\n",
      " [7.28858592e-07]\n",
      " [7.28195437e-07]\n",
      " [7.27509473e-07]] [[4.56712121e-07]\n",
      " [4.56712121e-07]\n",
      " [4.56712121e-07]\n",
      " [4.56712122e-07]]\n",
      "[[1.09385068e-06]\n",
      " [1.09323898e-06]\n",
      " [1.09259298e-06]\n",
      " [1.09191273e-06]] [[4.56711988e-07]\n",
      " [4.56711988e-07]\n",
      " [4.56711988e-07]\n",
      " [4.56711989e-07]]\n",
      "[[2.68360654e-07]\n",
      " [2.68129312e-07]\n",
      " [2.67889557e-07]\n",
      " [2.67641411e-07]] [[1.68017211e-07]\n",
      " [1.68017211e-07]\n",
      " [1.68017211e-07]\n",
      " [1.68017212e-07]]\n",
      "[[4.02400015e-07]\n",
      " [4.02179188e-07]\n",
      " [4.01945743e-07]\n",
      " [4.01699700e-07]] [[1.68017193e-07]\n",
      " [1.68017193e-07]\n",
      " [1.68017193e-07]\n",
      " [1.68017194e-07]]\n",
      "[[2.68350071e-07]\n",
      " [2.68122935e-07]\n",
      " [2.67887386e-07]\n",
      " [2.67643447e-07]] [[1.68017175e-07]\n",
      " [1.68017175e-07]\n",
      " [1.68017175e-07]\n",
      " [1.68017176e-07]]\n",
      "[[4.02389414e-07]\n",
      " [4.02172793e-07]\n",
      " [4.01943554e-07]\n",
      " [4.01701717e-07]] [[1.68017157e-07]\n",
      " [1.68017157e-07]\n",
      " [1.68017157e-07]\n",
      " [1.68017158e-07]]\n",
      "[[5.36428728e-07]\n",
      " [5.36222622e-07]\n",
      " [5.35999692e-07]\n",
      " [5.35759959e-07]] [[1.68017139e-07]\n",
      " [1.68017139e-07]\n",
      " [1.68017139e-07]\n",
      " [1.68017140e-07]]\n",
      "[[6.70468014e-07]\n",
      " [6.70272423e-07]\n",
      " [6.70055802e-07]\n",
      " [6.69818172e-07]] [[1.68017121e-07]\n",
      " [1.68017121e-07]\n",
      " [1.68017122e-07]\n",
      " [1.68017122e-07]]\n",
      "[[8.04517786e-07]\n",
      " [8.04328504e-07]\n",
      " [8.04113986e-07]\n",
      " [8.03874253e-07]] [[1.68017104e-07]\n",
      " [1.68017103e-07]\n",
      " [1.68017104e-07]\n",
      " [1.68017104e-07]]\n",
      "[[6.70467871e-07]\n",
      " [6.70272280e-07]\n",
      " [6.70055659e-07]\n",
      " [6.69818029e-07]] [[1.68017086e-07]\n",
      " [1.68017086e-07]\n",
      " [1.68017086e-07]\n",
      " [1.68017086e-07]]\n",
      "[[5.36417984e-07]\n",
      " [5.36216084e-07]\n",
      " [5.35997360e-07]\n",
      " [5.35761833e-07]] [[1.68017068e-07]\n",
      " [1.68017068e-07]\n",
      " [1.68017068e-07]\n",
      " [1.68017068e-07]]\n",
      "[[4.02368126e-07]\n",
      " [4.02159917e-07]\n",
      " [4.01939090e-07]\n",
      " [4.01705666e-07]] [[1.6801705e-07]\n",
      " [1.6801705e-07]\n",
      " [1.6801705e-07]\n",
      " [1.6801705e-07]]\n",
      "[[2.68318297e-07]\n",
      " [2.68103778e-07]\n",
      " [2.67880848e-07]\n",
      " [2.67649527e-07]] [[1.68017032e-07]\n",
      " [1.68017032e-07]\n",
      " [1.68017032e-07]\n",
      " [1.68017032e-07]]\n",
      "[[4.02376453e-07]\n",
      " [4.02159831e-07]\n",
      " [4.01930591e-07]\n",
      " [4.01688755e-07]] [[1.68017014e-07]\n",
      " [1.68017014e-07]\n",
      " [1.68017014e-07]\n",
      " [1.68017014e-07]]\n",
      "[[2.68326652e-07]\n",
      " [2.68103721e-07]\n",
      " [2.67872378e-07]\n",
      " [2.67632646e-07]] [[1.68016996e-07]\n",
      " [1.68016996e-07]\n",
      " [1.68016996e-07]\n",
      " [1.68016996e-07]]\n",
      "[[1.34276881e-07]\n",
      " [1.34047640e-07]\n",
      " [1.33814194e-07]\n",
      " [1.33576565e-07]] [[1.68016978e-07]\n",
      " [1.68016978e-07]\n",
      " [1.68016978e-07]\n",
      " [1.68016978e-07]]\n",
      "[[2.68335008e-07]\n",
      " [2.68103664e-07]\n",
      " [2.67863908e-07]\n",
      " [2.67615764e-07]] [[1.6801696e-07]\n",
      " [1.6801696e-07]\n",
      " [1.6801696e-07]\n",
      " [1.6801696e-07]]\n",
      "[[1.34285265e-07]\n",
      " [1.34047611e-07]\n",
      " [1.33805753e-07]\n",
      " [1.33559712e-07]] [[1.68016942e-07]\n",
      " [1.68016942e-07]\n",
      " [1.68016942e-07]\n",
      " [1.68016942e-07]]\n",
      "[[2.68334950e-07]\n",
      " [2.68103606e-07]\n",
      " [2.67863851e-07]\n",
      " [2.67615707e-07]] [[1.68016924e-07]\n",
      " [1.68016924e-07]\n",
      " [1.68016924e-07]\n",
      " [1.68016924e-07]]\n",
      "[[1.34285236e-07]\n",
      " [1.34047583e-07]\n",
      " [1.33805724e-07]\n",
      " [1.33559683e-07]] [[1.68016906e-07]\n",
      " [1.68016906e-07]\n",
      " [1.68016906e-07]\n",
      " [1.68016906e-07]]\n",
      "[[ 2.35550216e-10]\n",
      " [-8.41228565e-12]\n",
      " [-2.52373996e-10]\n",
      " [-4.96311950e-10]] [[1.68016888e-07]\n",
      " [1.68016888e-07]\n",
      " [1.68016888e-07]\n",
      " [1.68016888e-07]]\n",
      "[[-1.33814107e-07]\n",
      " [-1.34064379e-07]\n",
      " [-1.34310443e-07]\n",
      " [-1.34552278e-07]] [[1.6801687e-07]\n",
      " [1.6801687e-07]\n",
      " [1.6801687e-07]\n",
      " [1.6801687e-07]]\n",
      "[[ 2.43962694e-10]\n",
      " [-8.41228025e-12]\n",
      " [-2.60786463e-10]\n",
      " [-5.13136099e-10]] [[1.68016852e-07]\n",
      " [1.68016852e-07]\n",
      " [1.68016852e-07]\n",
      " [1.68016852e-07]]\n",
      "[[1.34283076e-07]\n",
      " [1.34041216e-07]\n",
      " [1.33795151e-07]\n",
      " [1.33544905e-07]] [[1.68016834e-07]\n",
      " [1.68016834e-07]\n",
      " [1.68016834e-07]\n",
      " [1.68016834e-07]]\n",
      "[[2.68322161e-07]\n",
      " [2.68090816e-07]\n",
      " [2.67851060e-07]\n",
      " [2.67602917e-07]] [[1.68016816e-07]\n",
      " [1.68016816e-07]\n",
      " [1.68016816e-07]\n",
      " [1.68016816e-07]]\n",
      "[[1.34272533e-07]\n",
      " [1.34034878e-07]\n",
      " [1.33793020e-07]\n",
      " [1.33546980e-07]] [[1.68016798e-07]\n",
      " [1.68016798e-07]\n",
      " [1.68016798e-07]\n",
      " [1.68016798e-07]]\n",
      "[[ 2.22933286e-10]\n",
      " [-2.10306016e-11]\n",
      " [-2.64992510e-10]\n",
      " [-5.08929474e-10]] [[1.6801678e-07]\n",
      " [1.6801678e-07]\n",
      " [1.6801678e-07]\n",
      " [1.6801678e-07]]\n",
      "[[-1.33826638e-07]\n",
      " [-1.34076911e-07]\n",
      " [-1.34322976e-07]\n",
      " [-1.34564810e-07]] [[1.68016762e-07]\n",
      " [1.68016762e-07]\n",
      " [1.68016762e-07]\n",
      " [1.68016762e-07]]\n",
      "[[-2.67876180e-07]\n",
      " [-2.68132762e-07]\n",
      " [-2.68380931e-07]\n",
      " [-2.68620662e-07]] [[1.68016744e-07]\n",
      " [1.68016744e-07]\n",
      " [1.68016744e-07]\n",
      " [1.68016744e-07]]\n"
     ]
    }
   ],
   "source": [
    "knl = GPy.kern.RBF(input_dim = 1, variance=1, lengthscale=1)\n",
    "actions = train_data['test05022023'].port.to_numpy().reshape(-1, 100)\n",
    "rewards = train_data['test05022023'].reward.to_numpy().reshape(-1, 100)\n",
    "for trial in range(1, 101):\n",
    "    gp = GPy.models.GPClassification(actions[0, :trial].reshape(-1, 1), rewards[0, :trial].reshape(-1, 1), kernel = knl, inference_method = GPy.inference.latent_function_inference.expectation_propagation.EP())\n",
    "    gp.optimize()\n",
    "    mu, var = gp.predict_noiseless(np.array([1,2,3,4]).reshape(-1, 1))\n",
    "    print(mu, var)\n",
    "    # print(gp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3fc1e9",
   "metadata": {},
   "source": [
    "## GPflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90be9c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dlab\\rishika_sim\\utils\\dfLoading.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sess_block'] = 0\n",
      "c:\\Users\\dlab\\rishika_sim\\utils\\dfLoading.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sess_block'] = df.groupby(['animal', 'task']).sess_block.cumsum()\n",
      "c:\\Users\\dlab\\rishika_sim\\utils\\dfLoading.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['block_group'] = df.groupby(['animal', 'task', 'session']).ngroup()\n",
      "c:\\Users\\dlab\\rishika_sim\\utils\\dfLoading.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['block_group'] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gpflow\n",
    "from scipy.special import expit\n",
    "from scipy.optimize import minimize\n",
    "train_data, test_data = load_data_into_dict()\n",
    "\n",
    "actions = train_data['test05022023'].port.to_numpy().reshape(-1, 100)\n",
    "rewards = train_data['test05022023'].reward.to_numpy().reshape(-1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81ddd670",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GPflow\n",
    "\n",
    "def nLL_gp_ucb_gpflow(x0, df, arms):\n",
    "    beta, tau, ls, lr_sig = x0\n",
    "    ll = 0\n",
    "    sessions = df.session.value_counts()\n",
    "    chosen_action = df.port.to_numpy()\n",
    "    rewarded = df.reward.to_numpy()\n",
    "    p = np.ones(len(df))*1e-7\n",
    "    session_ends = np.cumsum(sessions)\n",
    "    \n",
    "    sess_num = 0\n",
    "    sess_start = True\n",
    "    lik = gpflow.likelihoods.Bernoulli()\n",
    "\n",
    "    # gp settings\n",
    "    X = np.linspace(1,arms,arms).reshape(arms,1)\n",
    "    knl = gpflow.kernels.SquaredExponential(variance=lr_sig, lengthscales=ls)\n",
    "\n",
    "    for trial in range(len(chosen_action)):\n",
    "        # check if sess restarted\n",
    "        if sess_start == True:\n",
    "            # if group.block_group.unique()[0] == 1:\n",
    "            start_trial = trial\n",
    "\n",
    "        sess_start = False\n",
    "        \n",
    "        if len(np.unique(rewarded[start_trial:trial])) > 1:\n",
    "        # what actions were taken so far\n",
    "            a = chosen_action[start_trial:trial]\n",
    "\n",
    "            # what rewards were given for each action\n",
    "            r = rewarded[start_trial:trial]\n",
    "\n",
    "            # update gp using all the info we got so far on actions and rew\n",
    "            gp = gpflow.models.VGP(\n",
    "                (a.reshape(-1, 1), r.reshape(-1, 1)),\n",
    "                kernel = knl,\n",
    "                likelihood = lik\n",
    "            )\n",
    "            opt = gpflow.optimizers.Scipy()\n",
    "            opt.minimize(gp.training_loss, gp.trainable_variables)\n",
    "            \n",
    "            mu, var = gp.predict_f(X)\n",
    "            # print(mu, var)\n",
    "            mu_star = expit(mu)\n",
    "            sd_star = expit(mu+np.sqrt(var)) - expit(mu)\n",
    "\n",
    "            # calculate probability of taking any action\n",
    "            input_to_softmax = (mu_star + beta*sd_star)/tau\n",
    "            P = np.exp(input_to_softmax - max(input_to_softmax))\n",
    "            P = P/ np.sum(P)\n",
    "            P = np.clip(P, a_min = 1e-7, a_max = None)\n",
    "\n",
    "            # what was probability of action taken?\n",
    "            chosen = int(chosen_action[trial]-1)\n",
    "            p[trial] = P[chosen]\n",
    "\n",
    "        # check how many trials elapsed\n",
    "        if trial == session_ends.iloc[sess_num]:\n",
    "            sess_start=True\n",
    "            sess_num += 1\n",
    "    p[p == 1e-7] = np.nan\n",
    "    ll += np.nansum(np.log(p))\n",
    "    nll = -ll\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "462dda0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dlab\\AppData\\Local\\Temp\\ipykernel_20488\\769113110.py:57: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  p[trial] = P[chosen]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 45 calls to <function Scipy.eval_func.<locals>._tf_eval at 0x000001A6ADDE1870> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Scipy.eval_func.<locals>._tf_eval at 0x000001A6ADDE2F80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7347.770040366276"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nLL_gp_ucb_gpflow([0.1, 1, 1, 2], train_data['test05022023'], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71b840e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.44435427]\n",
      " [-1.99145769]\n",
      " [-3.28335865]\n",
      " [-1.99145769]], shape=(4, 1), dtype=float64) mean trial 2\n",
      "tf.Tensor(\n",
      "[[14.19826143]\n",
      " [10.42980172]\n",
      " [ 3.61524928]\n",
      " [10.42980172]], shape=(4, 1), dtype=float64) var trial 2\n",
      "tf.Tensor(\n",
      "[[-4.42414868]\n",
      " [-4.42414869]\n",
      " [-4.42414869]\n",
      " [-4.42414867]], shape=(4, 1), dtype=float64) mean trial 3\n",
      "tf.Tensor(\n",
      "[[4.81339482]\n",
      " [4.81339472]\n",
      " [4.81339474]\n",
      " [4.8133949 ]], shape=(4, 1), dtype=float64) var trial 3\n",
      "tf.Tensor(\n",
      "[[-5.20576098]\n",
      " [-5.20576099]\n",
      " [-5.20576099]\n",
      " [-5.20576098]], shape=(4, 1), dtype=float64) mean trial 4\n",
      "tf.Tensor(\n",
      "[[5.64754031]\n",
      " [5.64754013]\n",
      " [5.64754009]\n",
      " [5.64754021]], shape=(4, 1), dtype=float64) var trial 4\n",
      "tf.Tensor(\n",
      "[[-1.56619414e-04]\n",
      " [-1.18306967e-01]\n",
      " [-9.98148921e-01]\n",
      " [-1.18311757e-01]], shape=(4, 1), dtype=float64) mean trial 5\n",
      "tf.Tensor(\n",
      "[[0.44063165]\n",
      " [0.93490465]\n",
      " [0.46081184]\n",
      " [0.94204368]], shape=(4, 1), dtype=float64) var trial 5\n",
      "tf.Tensor(\n",
      "[[-1.71860880e-04]\n",
      " [-1.91320878e-01]\n",
      " [-1.44924977e+00]\n",
      " [-1.91356348e-01]], shape=(4, 1), dtype=float64) mean trial 6\n",
      "tf.Tensor(\n",
      "[[0.55021033]\n",
      " [1.58584291]\n",
      " [0.59446821]\n",
      " [1.60452876]], shape=(4, 1), dtype=float64) var trial 6\n",
      "tf.Tensor(\n",
      "[[-1.66295705e-04]\n",
      " [-2.15186556e-01]\n",
      " [-1.73920739e+00]\n",
      " [-2.15216441e-01]], shape=(4, 1), dtype=float64) mean trial 7\n",
      "tf.Tensor(\n",
      "[[0.60245631]\n",
      " [2.09939829]\n",
      " [0.66410597]\n",
      " [2.12302143]], shape=(4, 1), dtype=float64) var trial 7\n",
      "tf.Tensor(\n",
      "[[-1.18112478e-04]\n",
      " [-2.28717636e-01]\n",
      " [-1.95031045e+00]\n",
      " [-2.28747071e-01]], shape=(4, 1), dtype=float64) mean trial 8\n",
      "tf.Tensor(\n",
      "[[0.63377017]\n",
      " [2.5204723 ]\n",
      " [0.70806855]\n",
      " [2.54713869]], shape=(4, 1), dtype=float64) var trial 8\n",
      "tf.Tensor(\n",
      "[[-8.83485350e-05]\n",
      " [-2.11388329e-01]\n",
      " [-2.11501702e+00]\n",
      " [-2.11400598e-01]], shape=(4, 1), dtype=float64) mean trial 9\n",
      "tf.Tensor(\n",
      "[[0.65498158]\n",
      " [2.88872193]\n",
      " [0.7388126 ]\n",
      " [2.91147966]], shape=(4, 1), dtype=float64) var trial 9\n",
      "tf.Tensor(\n",
      "[[ 0.40174291]\n",
      " [-0.02334655]\n",
      " [-2.11314972]\n",
      " [-0.02882702]], shape=(4, 1), dtype=float64) mean trial 10\n",
      "tf.Tensor(\n",
      "[[0.488213  ]\n",
      " [2.92539712]\n",
      " [0.73750031]\n",
      " [2.92585083]], shape=(4, 1), dtype=float64) var trial 10\n",
      "tf.Tensor(\n",
      "[[ 6.47856574e-01]\n",
      " [-1.88264143e-04]\n",
      " [-2.17541811e+00]\n",
      " [-2.68109150e-04]], shape=(4, 1), dtype=float64) mean trial 11\n",
      "tf.Tensor(\n",
      "[[0.41647516]\n",
      " [3.17560528]\n",
      " [0.78255291]\n",
      " [3.17560532]], shape=(4, 1), dtype=float64) var trial 11\n",
      "tf.Tensor(\n",
      "[[ 6.56357771e-01]\n",
      " [-9.83417580e-05]\n",
      " [-2.30474118e+00]\n",
      " [-1.37499746e-04]], shape=(4, 1), dtype=float64) mean trial 12\n",
      "tf.Tensor(\n",
      "[[0.42262751]\n",
      " [3.48395671]\n",
      " [0.80262669]\n",
      " [3.48395673]], shape=(4, 1), dtype=float64) var trial 12\n",
      "tf.Tensor(\n",
      "[[ 6.62851501e-01]\n",
      " [-5.67273529e-05]\n",
      " [-2.41391849e+00]\n",
      " [-7.82010093e-05]], shape=(4, 1), dtype=float64) mean trial 13\n",
      "tf.Tensor(\n",
      "[[0.42737424]\n",
      " [3.75614901]\n",
      " [0.81831993]\n",
      " [3.75614902]], shape=(4, 1), dtype=float64) var trial 13\n",
      "tf.Tensor(\n",
      "[[ 6.68059312e-01]\n",
      " [-1.25256567e-04]\n",
      " [-2.50801023e+00]\n",
      " [-1.70735398e-04]], shape=(4, 1), dtype=float64) mean trial 14\n",
      "tf.Tensor(\n",
      "[[0.43115578]\n",
      " [3.99933581]\n",
      " [0.83108451]\n",
      " [3.99933583]], shape=(4, 1), dtype=float64) var trial 14\n",
      "tf.Tensor(\n",
      "[[ 6.72304774e-01]\n",
      " [-5.78523956e-05]\n",
      " [-2.59053109e+00]\n",
      " [-7.81286484e-05]], shape=(4, 1), dtype=float64) mean trial 15\n",
      "tf.Tensor(\n",
      "[[0.43428246]\n",
      " [4.21941143]\n",
      " [0.84167745]\n",
      " [4.21941143]], shape=(4, 1), dtype=float64) var trial 15\n",
      "tf.Tensor(\n",
      "[[ 8.48235057e-01]\n",
      " [-2.44531998e-07]\n",
      " [-2.64319901e+00]\n",
      " [-3.60088978e-07]], shape=(4, 1), dtype=float64) mean trial 16\n",
      "tf.Tensor(\n",
      "[[0.38740839]\n",
      " [4.48796702]\n",
      " [0.88233725]\n",
      " [4.48796702]], shape=(4, 1), dtype=float64) var trial 16\n",
      "tf.Tensor(\n",
      "[[ 8.51749833e-01]\n",
      " [-3.72214461e-07]\n",
      " [-2.71463788e+00]\n",
      " [-5.42398389e-07]], shape=(4, 1), dtype=float64) mean trial 17\n",
      "tf.Tensor(\n",
      "[[0.38944722]\n",
      " [4.68705658]\n",
      " [0.88993168]\n",
      " [4.68705658]], shape=(4, 1), dtype=float64) var trial 17\n",
      "tf.Tensor(\n",
      "[[ 8.54778300e-01]\n",
      " [-1.49566571e-06]\n",
      " [-2.77887483e+00]\n",
      " [-2.16011397e-06]], shape=(4, 1), dtype=float64) mean trial 18\n",
      "tf.Tensor(\n",
      "[[0.39119366]\n",
      " [4.87024925]\n",
      " [0.89644644]\n",
      " [4.87024925]], shape=(4, 1), dtype=float64) var trial 18\n",
      "tf.Tensor(\n",
      "[[ 9.88940901e-01]\n",
      " [-2.60976520e-10]\n",
      " [-2.82456038e+00]\n",
      " [-4.01577749e-10]], shape=(4, 1), dtype=float64) mean trial 19\n",
      "tf.Tensor(\n",
      "[[0.35891938]\n",
      " [5.12394515]\n",
      " [0.93270915]\n",
      " [5.12394515]], shape=(4, 1), dtype=float64) var trial 19\n",
      "tf.Tensor(\n",
      "[[ 5.65917544e-01]\n",
      " [-1.03103556e-03]\n",
      " [-2.64976872e+00]\n",
      " [-1.31103690e-03]], shape=(4, 1), dtype=float64) mean trial 20\n",
      "tf.Tensor(\n",
      "[[0.24389237]\n",
      " [4.19169736]\n",
      " [0.79807351]\n",
      " [4.19169833]], shape=(4, 1), dtype=float64) var trial 20\n",
      "tf.Tensor(\n",
      "[[ 0.51121018]\n",
      " [-1.65433606]\n",
      " [-2.90927931]\n",
      " [-1.77809158]], shape=(4, 1), dtype=float64) mean trial 21\n",
      "tf.Tensor(\n",
      "[[0.23919168]\n",
      " [1.23909845]\n",
      " [0.94898324]\n",
      " [3.21543317]], shape=(4, 1), dtype=float64) var trial 21\n",
      "tf.Tensor(\n",
      "[[ 0.62907336]\n",
      " [-1.65569227]\n",
      " [-2.96199726]\n",
      " [-1.76004816]], shape=(4, 1), dtype=float64) mean trial 22\n",
      "tf.Tensor(\n",
      "[[0.2211257 ]\n",
      " [1.3107735 ]\n",
      " [0.99104137]\n",
      " [3.41994528]], shape=(4, 1), dtype=float64) var trial 22\n",
      "tf.Tensor(\n",
      "[[ 0.72687687]\n",
      " [-1.66159005]\n",
      " [-3.01004796]\n",
      " [-1.74901863]], shape=(4, 1), dtype=float64) mean trial 23\n",
      "tf.Tensor(\n",
      "[[0.20722305]\n",
      " [1.37341062]\n",
      " [1.03026796]\n",
      " [3.60643452]], shape=(4, 1), dtype=float64) var trial 23\n",
      "tf.Tensor(\n",
      "[[ 0.48550588]\n",
      " [-1.63729393]\n",
      " [-2.87563797]\n",
      " [-1.76806078]], shape=(4, 1), dtype=float64) mean trial 24\n",
      "tf.Tensor(\n",
      "[[0.1675036 ]\n",
      " [1.19211305]\n",
      " [0.9226631 ]\n",
      " [3.10692893]], shape=(4, 1), dtype=float64) var trial 24\n",
      "tf.Tensor(\n",
      "[[ 0.31125224]\n",
      " [-1.63727323]\n",
      " [-2.79338172]\n",
      " [-1.79515335]], shape=(4, 1), dtype=float64) mean trial 25\n",
      "tf.Tensor(\n",
      "[[0.14469042]\n",
      " [1.07067107]\n",
      " [0.86001649]\n",
      " [2.80441935]], shape=(4, 1), dtype=float64) var trial 25\n",
      "tf.Tensor(\n",
      "[[ 0.39725076]\n",
      " [-1.63075478]\n",
      " [-2.82592253]\n",
      " [-1.77592149]], shape=(4, 1), dtype=float64) mean trial 26\n",
      "tf.Tensor(\n",
      "[[0.13607024]\n",
      " [1.12140783]\n",
      " [0.88449063]\n",
      " [2.92976606]], shape=(4, 1), dtype=float64) var trial 26\n",
      "tf.Tensor(\n",
      "[[ 0.47237833]\n",
      " [-1.62849222]\n",
      " [-2.85754769]\n",
      " [-1.76230328]], shape=(4, 1), dtype=float64) mean trial 27\n",
      "tf.Tensor(\n",
      "[[0.1289961 ]\n",
      " [1.16715336]\n",
      " [0.90871625]\n",
      " [3.04989516]], shape=(4, 1), dtype=float64) var trial 27\n",
      "tf.Tensor(\n",
      "[[ 0.53892685]\n",
      " [-1.62846168]\n",
      " [-2.88769794]\n",
      " [-1.75220014]], shape=(4, 1), dtype=float64) mean trial 28\n",
      "tf.Tensor(\n",
      "[[0.12303843]\n",
      " [1.20924794]\n",
      " [0.93202602]\n",
      " [3.16325582]], shape=(4, 1), dtype=float64) var trial 28\n",
      "tf.Tensor(\n",
      "[[ 0.59855284]\n",
      " [-1.62981935]\n",
      " [-2.91535711]\n",
      " [-1.74310442]], shape=(4, 1), dtype=float64) mean trial 29\n",
      "tf.Tensor(\n",
      "[[0.11794901]\n",
      " [1.24766163]\n",
      " [0.95374901]\n",
      " [3.26861676]], shape=(4, 1), dtype=float64) var trial 29\n",
      "tf.Tensor(\n",
      "[[ 0.65246292]\n",
      " [-1.63277643]\n",
      " [-2.94228282]\n",
      " [-1.73759973]], shape=(4, 1), dtype=float64) mean trial 30\n",
      "tf.Tensor(\n",
      "[[0.11352633]\n",
      " [1.28334445]\n",
      " [0.97521082]\n",
      " [3.36950423]], shape=(4, 1), dtype=float64) var trial 30\n",
      "tf.Tensor(\n",
      "[[ 0.70164337]\n",
      " [-1.6368372 ]\n",
      " [-2.96803328]\n",
      " [-1.73363392]], shape=(4, 1), dtype=float64) mean trial 31\n",
      "tf.Tensor(\n",
      "[[0.10963246]\n",
      " [1.31629287]\n",
      " [0.99597674]\n",
      " [3.46633491]], shape=(4, 1), dtype=float64) var trial 31\n",
      "tf.Tensor(\n",
      "[[ 0.74676547]\n",
      " [-1.64134318]\n",
      " [-2.99209664]\n",
      " [-1.72993464]], shape=(4, 1), dtype=float64) mean trial 32\n",
      "tf.Tensor(\n",
      "[[0.10619891]\n",
      " [1.3474796 ]\n",
      " [1.01555795]\n",
      " [3.55825884]], shape=(4, 1), dtype=float64) var trial 32\n",
      "tf.Tensor(\n",
      "[[ 0.7884345 ]\n",
      " [-1.64614875]\n",
      " [-3.0149785 ]\n",
      " [-1.72731707]], shape=(4, 1), dtype=float64) mean trial 33\n",
      "tf.Tensor(\n",
      "[[0.10312291]\n",
      " [1.37655505]\n",
      " [1.03434562]\n",
      " [3.64562417]], shape=(4, 1), dtype=float64) var trial 33\n",
      "tf.Tensor(\n",
      "[[ 0.82710989]\n",
      " [-1.65146211]\n",
      " [-3.03692572]\n",
      " [-1.72492876]], shape=(4, 1), dtype=float64) mean trial 34\n",
      "tf.Tensor(\n",
      "[[0.10035623]\n",
      " [1.40472633]\n",
      " [1.05252857]\n",
      " [3.72953656]], shape=(4, 1), dtype=float64) var trial 34\n",
      "tf.Tensor(\n",
      "[[ 0.69651444]\n",
      " [-1.63094813]\n",
      " [-2.95852408]\n",
      " [-1.72999995]], shape=(4, 1), dtype=float64) mean trial 35\n",
      "tf.Tensor(\n",
      "[[0.08879984]\n",
      " [1.3034489 ]\n",
      " [0.98835152]\n",
      " [3.43508836]], shape=(4, 1), dtype=float64) var trial 35\n",
      "tf.Tensor(\n",
      "[[ 0.73326421]\n",
      " [-1.63494665]\n",
      " [-2.97867996]\n",
      " [-1.72712737]], shape=(4, 1), dtype=float64) mean trial 36\n",
      "tf.Tensor(\n",
      "[[0.08647432]\n",
      " [1.32959974]\n",
      " [1.00456908]\n",
      " [3.51129111]], shape=(4, 1), dtype=float64) var trial 36\n",
      "tf.Tensor(\n",
      "[[ 0.76767124]\n",
      " [-1.63936276]\n",
      " [-2.99776386]\n",
      " [-1.72454401]], shape=(4, 1), dtype=float64) mean trial 37\n",
      "tf.Tensor(\n",
      "[[0.08436796]\n",
      " [1.35453863]\n",
      " [1.02016356]\n",
      " [3.58308062]], shape=(4, 1), dtype=float64) var trial 37\n",
      "tf.Tensor(\n",
      "[[ 0.79998491]\n",
      " [-1.64342778]\n",
      " [-3.0155292 ]\n",
      " [-1.72220341]], shape=(4, 1), dtype=float64) mean trial 38\n",
      "tf.Tensor(\n",
      "[[0.08243964]\n",
      " [1.37749125]\n",
      " [1.03502964]\n",
      " [3.65290756]], shape=(4, 1), dtype=float64) var trial 38\n",
      "tf.Tensor(\n",
      "[[ 0.83043694]\n",
      " [-1.64770332]\n",
      " [-3.03363173]\n",
      " [-1.72241776]], shape=(4, 1), dtype=float64) mean trial 39\n",
      "tf.Tensor(\n",
      "[[0.08066098]\n",
      " [1.3989541 ]\n",
      " [1.04996099]\n",
      " [3.72053991]], shape=(4, 1), dtype=float64) var trial 39\n",
      "tf.Tensor(\n",
      "[[ 0.85923535]\n",
      " [-1.65186525]\n",
      " [-3.0500733 ]\n",
      " [-1.72082096]], shape=(4, 1), dtype=float64) mean trial 40\n",
      "tf.Tensor(\n",
      "[[0.07902093]\n",
      " [1.41972959]\n",
      " [1.06366191]\n",
      " [3.7836966 ]], shape=(4, 1), dtype=float64) var trial 40\n",
      "tf.Tensor(\n",
      "[[ 0.88656175]\n",
      " [-1.65668388]\n",
      " [-3.06628001]\n",
      " [-1.71975989]], shape=(4, 1), dtype=float64) mean trial 41\n",
      "tf.Tensor(\n",
      "[[0.07750509]\n",
      " [1.44042941]\n",
      " [1.07729178]\n",
      " [3.84769378]], shape=(4, 1), dtype=float64) var trial 41\n",
      "tf.Tensor(\n",
      "[[ 0.91250613]\n",
      " [-1.66135349]\n",
      " [-3.08213405]\n",
      " [-1.71904049]], shape=(4, 1), dtype=float64) mean trial 42\n",
      "tf.Tensor(\n",
      "[[0.07608657]\n",
      " [1.45958865]\n",
      " [1.09053102]\n",
      " [3.90783312]], shape=(4, 1), dtype=float64) var trial 42\n",
      "tf.Tensor(\n",
      "[[ 0.93719935]\n",
      " [-1.66634093]\n",
      " [-3.097443  ]\n",
      " [-1.71868731]], shape=(4, 1), dtype=float64) mean trial 43\n",
      "tf.Tensor(\n",
      "[[0.07476253]\n",
      " [1.47929775]\n",
      " [1.10360839]\n",
      " [3.96833557]], shape=(4, 1), dtype=float64) var trial 43\n",
      "tf.Tensor(\n",
      "[[ 0.83269879]\n",
      " [-1.64504566]\n",
      " [-3.03099136]\n",
      " [-1.71943847]], shape=(4, 1), dtype=float64) mean trial 44\n",
      "tf.Tensor(\n",
      "[[0.06743976]\n",
      " [1.39551873]\n",
      " [1.04762981]\n",
      " [3.71318321]], shape=(4, 1), dtype=float64) var trial 44\n",
      "tf.Tensor(\n",
      "[[ 0.85659911]\n",
      " [-1.6485226 ]\n",
      " [-3.04484589]\n",
      " [-1.71897068]], shape=(4, 1), dtype=float64) mean trial 45\n",
      "tf.Tensor(\n",
      "[[0.06627655]\n",
      " [1.41254823]\n",
      " [1.0593388 ]\n",
      " [3.76616246]], shape=(4, 1), dtype=float64) var trial 45\n",
      "tf.Tensor(\n",
      "[[ 0.87960851]\n",
      " [-1.65264434]\n",
      " [-3.05869918]\n",
      " [-1.71813941]], shape=(4, 1), dtype=float64) mean trial 46\n",
      "tf.Tensor(\n",
      "[[0.06520021]\n",
      " [1.43021554]\n",
      " [1.07072293]\n",
      " [3.81953908]], shape=(4, 1), dtype=float64) var trial 46\n",
      "tf.Tensor(\n",
      "[[ 0.90157775]\n",
      " [-1.65620807]\n",
      " [-3.07147245]\n",
      " [-1.71721061]], shape=(4, 1), dtype=float64) mean trial 47\n",
      "tf.Tensor(\n",
      "[[0.06418231]\n",
      " [1.44661709]\n",
      " [1.08168949]\n",
      " [3.86985828]], shape=(4, 1), dtype=float64) var trial 47\n",
      "tf.Tensor(\n",
      "[[ 0.92267063]\n",
      " [-1.66083862]\n",
      " [-3.08496525]\n",
      " [-1.71734174]], shape=(4, 1), dtype=float64) mean trial 48\n",
      "tf.Tensor(\n",
      "[[0.06321824]\n",
      " [1.46270893]\n",
      " [1.09295862]\n",
      " [3.92132326]], shape=(4, 1), dtype=float64) var trial 48\n",
      "tf.Tensor(\n",
      "[[ 0.94287517]\n",
      " [-1.66398638]\n",
      " [-3.09680384]\n",
      " [-1.71632754]], shape=(4, 1), dtype=float64) mean trial 49\n",
      "tf.Tensor(\n",
      "[[0.06230296]\n",
      " [1.47765415]\n",
      " [1.10331549]\n",
      " [3.96801095]], shape=(4, 1), dtype=float64) var trial 49\n",
      "tf.Tensor(\n",
      "[[ 0.96233291]\n",
      " [-1.66816891]\n",
      " [-3.10920579]\n",
      " [-1.71688423]], shape=(4, 1), dtype=float64) mean trial 50\n",
      "tf.Tensor(\n",
      "[[0.06144729]\n",
      " [1.49263272]\n",
      " [1.11379763]\n",
      " [4.01762372]], shape=(4, 1), dtype=float64) var trial 50\n",
      "tf.Tensor(\n",
      "[[ 0.98105347]\n",
      " [-1.6723088 ]\n",
      " [-3.12115102]\n",
      " [-1.7161898 ]], shape=(4, 1), dtype=float64) mean trial 51\n",
      "tf.Tensor(\n",
      "[[0.06063987]\n",
      " [1.50800044]\n",
      " [1.12400166]\n",
      " [4.06499642]], shape=(4, 1), dtype=float64) var trial 51\n",
      "tf.Tensor(\n",
      "[[ 0.99915458]\n",
      " [-1.67544951]\n",
      " [-3.13209539]\n",
      " [-1.71623969]], shape=(4, 1), dtype=float64) mean trial 52\n",
      "tf.Tensor(\n",
      "[[0.05981354]\n",
      " [1.52061595]\n",
      " [1.13376803]\n",
      " [4.10604138]], shape=(4, 1), dtype=float64) var trial 52\n",
      "tf.Tensor(\n",
      "[[ 1.0165985 ]\n",
      " [-1.68110154]\n",
      " [-3.14539825]\n",
      " [-1.71776503]], shape=(4, 1), dtype=float64) mean trial 53\n",
      "tf.Tensor(\n",
      "[[0.05911826]\n",
      " [1.5367538 ]\n",
      " [1.14517156]\n",
      " [4.15961531]], shape=(4, 1), dtype=float64) var trial 53\n",
      "tf.Tensor(\n",
      "[[ 1.0334668 ]\n",
      " [-1.68396132]\n",
      " [-3.1546595 ]\n",
      " [-1.7171264 ]], shape=(4, 1), dtype=float64) mean trial 54\n",
      "tf.Tensor(\n",
      "[[0.05840258]\n",
      " [1.54805969]\n",
      " [1.15274453]\n",
      " [4.19592133]], shape=(4, 1), dtype=float64) var trial 54\n",
      "tf.Tensor(\n",
      "[[ 1.04972957]\n",
      " [-1.68836329]\n",
      " [-3.16561228]\n",
      " [-1.71698424]], shape=(4, 1), dtype=float64) mean trial 55\n",
      "tf.Tensor(\n",
      "[[0.05773382]\n",
      " [1.56217358]\n",
      " [1.16259357]\n",
      " [4.24023796]], shape=(4, 1), dtype=float64) var trial 55\n",
      "tf.Tensor(\n",
      "[[ 1.06549793]\n",
      " [-1.69157466]\n",
      " [-3.1756622 ]\n",
      " [-1.71689372]], shape=(4, 1), dtype=float64) mean trial 56\n",
      "tf.Tensor(\n",
      "[[0.05709528]\n",
      " [1.57490787]\n",
      " [1.17138861]\n",
      " [4.28079023]], shape=(4, 1), dtype=float64) var trial 56\n",
      "tf.Tensor(\n",
      "[[ 0.9794703 ]\n",
      " [-1.66919157]\n",
      " [-3.11737029]\n",
      " [-1.71565432]], shape=(4, 1), dtype=float64) mean trial 57\n",
      "tf.Tensor(\n",
      "[[0.05216438]\n",
      " [1.50193532]\n",
      " [1.12079622]\n",
      " [4.05018782]], shape=(4, 1), dtype=float64) var trial 57\n",
      "tf.Tensor(\n",
      "[[ 0.99507183]\n",
      " [-1.67284837]\n",
      " [-3.12681936]\n",
      " [-1.71465807]], shape=(4, 1), dtype=float64) mean trial 58\n",
      "tf.Tensor(\n",
      "[[0.05159989]\n",
      " [1.51415235]\n",
      " [1.12879781]\n",
      " [4.08780213]], shape=(4, 1), dtype=float64) var trial 58\n",
      "tf.Tensor(\n",
      "[[ 0.91968638]\n",
      " [-1.65597588]\n",
      " [-3.07871895]\n",
      " [-1.71483392]], shape=(4, 1), dtype=float64) mean trial 59\n",
      "tf.Tensor(\n",
      "[[0.04775653]\n",
      " [1.4537772 ]\n",
      " [1.08777289]\n",
      " [3.89901966]], shape=(4, 1), dtype=float64) var trial 59\n",
      "tf.Tensor(\n",
      "[[ 0.92000393]\n",
      " [-1.68619874]\n",
      " [-3.13401975]\n",
      " [-1.76056994]], shape=(4, 1), dtype=float64) mean trial 60\n",
      "tf.Tensor(\n",
      "[[0.0478045 ]\n",
      " [1.47788817]\n",
      " [1.09276531]\n",
      " [3.98531093]], shape=(4, 1), dtype=float64) var trial 60\n",
      "tf.Tensor(\n",
      "[[ 0.92029524]\n",
      " [-1.71252389]\n",
      " [-3.18318914]\n",
      " [-1.80122559]], shape=(4, 1), dtype=float64) mean trial 61\n",
      "tf.Tensor(\n",
      "[[0.04781866]\n",
      " [1.49755392]\n",
      " [1.09566912]\n",
      " [4.06031613]], shape=(4, 1), dtype=float64) var trial 61\n",
      "tf.Tensor(\n",
      "[[ 0.92058461]\n",
      " [-1.73857085]\n",
      " [-3.23152029]\n",
      " [-1.8416859 ]], shape=(4, 1), dtype=float64) mean trial 62\n",
      "tf.Tensor(\n",
      "[[0.04782424]\n",
      " [1.51779461]\n",
      " [1.10063891]\n",
      " [4.13698548]], shape=(4, 1), dtype=float64) var trial 62\n",
      "tf.Tensor(\n",
      "[[ 0.9207916 ]\n",
      " [-1.75975492]\n",
      " [-3.27203027]\n",
      " [-1.87490127]], shape=(4, 1), dtype=float64) mean trial 63\n",
      "tf.Tensor(\n",
      "[[0.04784282]\n",
      " [1.53469513]\n",
      " [1.10184164]\n",
      " [4.19814911]], shape=(4, 1), dtype=float64) var trial 63\n",
      "tf.Tensor(\n",
      "[[ 0.92098004]\n",
      " [-1.78041393]\n",
      " [-3.31192669]\n",
      " [-1.90944045]], shape=(4, 1), dtype=float64) mean trial 64\n",
      "tf.Tensor(\n",
      "[[0.04785582]\n",
      " [1.55114202]\n",
      " [1.10443059]\n",
      " [4.26010104]], shape=(4, 1), dtype=float64) var trial 64\n",
      "tf.Tensor(\n",
      "[[ 0.8536329 ]\n",
      " [-1.77179005]\n",
      " [-3.27354814]\n",
      " [-1.91217378]], shape=(4, 1), dtype=float64) mean trial 65\n",
      "tf.Tensor(\n",
      "[[0.04475617]\n",
      " [1.50123353]\n",
      " [1.07247355]\n",
      " [4.10215062]], shape=(4, 1), dtype=float64) var trial 65\n",
      "tf.Tensor(\n",
      "[[ 0.85376755]\n",
      " [-1.79067152]\n",
      " [-3.31031759]\n",
      " [-1.94399021]], shape=(4, 1), dtype=float64) mean trial 66\n",
      "tf.Tensor(\n",
      "[[0.04476835]\n",
      " [1.51497517]\n",
      " [1.07440843]\n",
      " [4.15846117]], shape=(4, 1), dtype=float64) var trial 66\n",
      "tf.Tensor(\n",
      "[[ 0.85392241]\n",
      " [-1.80986434]\n",
      " [-3.34542297]\n",
      " [-1.97326134]], shape=(4, 1), dtype=float64) mean trial 67\n",
      "tf.Tensor(\n",
      "[[0.04477252]\n",
      " [1.53118768]\n",
      " [1.07684561]\n",
      " [4.2159211 ]], shape=(4, 1), dtype=float64) var trial 67\n",
      "tf.Tensor(\n",
      "[[ 0.85408427]\n",
      " [-1.82818164]\n",
      " [-3.37849799]\n",
      " [-2.00079117]], shape=(4, 1), dtype=float64) mean trial 68\n",
      "tf.Tensor(\n",
      "[[0.04478604]\n",
      " [1.54553859]\n",
      " [1.07869508]\n",
      " [4.26725044]], shape=(4, 1), dtype=float64) var trial 68\n",
      "tf.Tensor(\n",
      "[[ 0.85153469]\n",
      " [-1.66318878]\n",
      " [-3.5553762 ]\n",
      " [-3.11689627]], shape=(4, 1), dtype=float64) mean trial 69\n",
      "tf.Tensor(\n",
      "[[0.04472228]\n",
      " [1.11107353]\n",
      " [1.15003079]\n",
      " [2.82350311]], shape=(4, 1), dtype=float64) var trial 69\n",
      "tf.Tensor(\n",
      "[[ 0.85134046]\n",
      " [-1.65352041]\n",
      " [-3.64658683]\n",
      " [-3.45451398]], shape=(4, 1), dtype=float64) mean trial 70\n",
      "tf.Tensor(\n",
      "[[0.04473029]\n",
      " [1.05371866]\n",
      " [1.1930816 ]\n",
      " [2.57820099]], shape=(4, 1), dtype=float64) var trial 70\n",
      "tf.Tensor(\n",
      "[[ 0.8513709 ]\n",
      " [-1.65573372]\n",
      " [-3.70745238]\n",
      " [-3.6556925 ]], shape=(4, 1), dtype=float64) mean trial 71\n",
      "tf.Tensor(\n",
      "[[0.04473921]\n",
      " [1.0328925 ]\n",
      " [1.22181643]\n",
      " [2.45492642]], shape=(4, 1), dtype=float64) var trial 71\n",
      "tf.Tensor(\n",
      "[[ 0.85142565]\n",
      " [-1.65652669]\n",
      " [-3.74588537]\n",
      " [-3.78506401]], shape=(4, 1), dtype=float64) mean trial 72\n",
      "tf.Tensor(\n",
      "[[0.04472715]\n",
      " [1.02123253]\n",
      " [1.23669952]\n",
      " [2.36141524]], shape=(4, 1), dtype=float64) var trial 72\n",
      "tf.Tensor(\n",
      "[[ 0.84519765]\n",
      " [-1.32442894]\n",
      " [-2.54101993]\n",
      " [-0.99355494]], shape=(4, 1), dtype=float64) mean trial 73\n",
      "tf.Tensor(\n",
      "[[0.04420893]\n",
      " [0.96574536]\n",
      " [0.47955844]\n",
      " [0.37387485]], shape=(4, 1), dtype=float64) var trial 73\n",
      "tf.Tensor(\n",
      "[[ 0.84554824]\n",
      " [-1.34276696]\n",
      " [-2.56809912]\n",
      " [-0.99533228]], shape=(4, 1), dtype=float64) mean trial 74\n",
      "tf.Tensor(\n",
      "[[0.04423109]\n",
      " [0.97945186]\n",
      " [0.47961126]\n",
      " [0.37522384]], shape=(4, 1), dtype=float64) var trial 74\n",
      "tf.Tensor(\n",
      "[[ 0.84713875]\n",
      " [-1.35682733]\n",
      " [-2.51958511]\n",
      " [-0.53130116]], shape=(4, 1), dtype=float64) mean trial 75\n",
      "tf.Tensor(\n",
      "[[0.04424521]\n",
      " [1.11362096]\n",
      " [0.46734328]\n",
      " [0.26045057]], shape=(4, 1), dtype=float64) var trial 75\n",
      "tf.Tensor(\n",
      "[[ 0.84748005]\n",
      " [-1.374678  ]\n",
      " [-2.54736215]\n",
      " [-0.53162863]], shape=(4, 1), dtype=float64) mean trial 76\n",
      "tf.Tensor(\n",
      "[[0.04427675]\n",
      " [1.13025409]\n",
      " [0.46800644]\n",
      " [0.26097097]], shape=(4, 1), dtype=float64) var trial 76\n",
      "tf.Tensor(\n",
      "[[ 0.84880291]\n",
      " [-1.38189461]\n",
      " [-2.54498225]\n",
      " [-0.2581122 ]], shape=(4, 1), dtype=float64) mean trial 77\n",
      "tf.Tensor(\n",
      "[[0.04431384]\n",
      " [1.23022559]\n",
      " [0.47353016]\n",
      " [0.21356721]], shape=(4, 1), dtype=float64) var trial 77\n",
      "tf.Tensor(\n",
      "[[ 0.84986335]\n",
      " [-1.38143645]\n",
      " [-2.55217196]\n",
      " [-0.06360305]], shape=(4, 1), dtype=float64) mean trial 78\n",
      "tf.Tensor(\n",
      "[[0.04435217]\n",
      " [1.30543132]\n",
      " [0.48171937]\n",
      " [0.18593875]], shape=(4, 1), dtype=float64) var trial 78\n",
      "tf.Tensor(\n",
      "[[ 0.85071642]\n",
      " [-1.37510932]\n",
      " [-2.56093419]\n",
      " [ 0.08666054]], shape=(4, 1), dtype=float64) mean trial 79\n",
      "tf.Tensor(\n",
      "[[0.04438553]\n",
      " [1.36148794]\n",
      " [0.48958968]\n",
      " [0.16729264]], shape=(4, 1), dtype=float64) var trial 79\n",
      "tf.Tensor(\n",
      "[[ 0.83356713]\n",
      " [-0.73909012]\n",
      " [-1.58439338]\n",
      " [ 0.0813574 ]], shape=(4, 1), dtype=float64) mean trial 80\n",
      "tf.Tensor(\n",
      "[[0.04318685]\n",
      " [0.75214144]\n",
      " [0.15360769]\n",
      " [0.15507258]], shape=(4, 1), dtype=float64) var trial 80\n",
      "tf.Tensor(\n",
      "[[ 0.83415783]\n",
      " [-0.75467621]\n",
      " [-1.60955301]\n",
      " [ 0.08085049]], shape=(4, 1), dtype=float64) mean trial 81\n",
      "tf.Tensor(\n",
      "[[0.04323106]\n",
      " [0.76364308]\n",
      " [0.15258794]\n",
      " [0.15550614]], shape=(4, 1), dtype=float64) var trial 81\n",
      "tf.Tensor(\n",
      "[[ 0.83478369]\n",
      " [-0.7698119 ]\n",
      " [-1.63346803]\n",
      " [ 0.08047133]], shape=(4, 1), dtype=float64) mean trial 82\n",
      "tf.Tensor(\n",
      "[[0.04327796]\n",
      " [0.77489713]\n",
      " [0.15163175]\n",
      " [0.15591461]], shape=(4, 1), dtype=float64) var trial 82\n",
      "tf.Tensor(\n",
      "[[ 0.8353054 ]\n",
      " [-0.78354673]\n",
      " [-1.65622309]\n",
      " [ 0.08014057]], shape=(4, 1), dtype=float64) mean trial 83\n",
      "tf.Tensor(\n",
      "[[0.04332174]\n",
      " [0.78539177]\n",
      " [0.15071018]\n",
      " [0.15630252]], shape=(4, 1), dtype=float64) var trial 83\n",
      "tf.Tensor(\n",
      "[[ 0.83582102]\n",
      " [-0.7973114 ]\n",
      " [-1.67792166]\n",
      " [ 0.07984158]], shape=(4, 1), dtype=float64) mean trial 84\n",
      "tf.Tensor(\n",
      "[[0.04335664]\n",
      " [0.796035  ]\n",
      " [0.14983709]\n",
      " [0.15665587]], shape=(4, 1), dtype=float64) var trial 84\n",
      "tf.Tensor(\n",
      "[[ 0.85129457]\n",
      " [-0.79744252]\n",
      " [-1.67966924]\n",
      " [ 0.08068115]], shape=(4, 1), dtype=float64) mean trial 85\n",
      "tf.Tensor(\n",
      "[[0.04287828]\n",
      " [0.80057141]\n",
      " [0.15024081]\n",
      " [0.15681932]], shape=(4, 1), dtype=float64) var trial 85\n",
      "tf.Tensor(\n",
      "[[ 0.85176224]\n",
      " [-0.81055324]\n",
      " [-1.70042239]\n",
      " [ 0.08037309]], shape=(4, 1), dtype=float64) mean trial 86\n",
      "tf.Tensor(\n",
      "[[0.04290837]\n",
      " [0.81098738]\n",
      " [0.14941822]\n",
      " [0.15714825]], shape=(4, 1), dtype=float64) var trial 86\n",
      "tf.Tensor(\n",
      "[[ 0.85219202]\n",
      " [-0.82294522]\n",
      " [-1.72024556]\n",
      " [ 0.0801635 ]], shape=(4, 1), dtype=float64) mean trial 87\n",
      "tf.Tensor(\n",
      "[[0.04294593]\n",
      " [0.82019976]\n",
      " [0.14863397]\n",
      " [0.15745303]], shape=(4, 1), dtype=float64) var trial 87\n",
      "tf.Tensor(\n",
      "[[ 0.86716505]\n",
      " [-0.82316378]\n",
      " [-1.72183022]\n",
      " [ 0.08093483]], shape=(4, 1), dtype=float64) mean trial 88\n",
      "tf.Tensor(\n",
      "[[0.04248871]\n",
      " [0.82511025]\n",
      " [0.14899794]\n",
      " [0.15761167]], shape=(4, 1), dtype=float64) var trial 88\n",
      "tf.Tensor(\n",
      "[[ 0.8676081 ]\n",
      " [-0.83514383]\n",
      " [-1.7408489 ]\n",
      " [ 0.0807046 ]], shape=(4, 1), dtype=float64) mean trial 89\n",
      "tf.Tensor(\n",
      "[[0.04250726]\n",
      " [0.83419851]\n",
      " [0.14822662]\n",
      " [0.15791037]], shape=(4, 1), dtype=float64) var trial 89\n",
      "tf.Tensor(\n",
      "[[ 0.85930933]\n",
      " [-0.64067286]\n",
      " [-1.43502888]\n",
      " [ 0.08637365]], shape=(4, 1), dtype=float64) mean trial 90\n",
      "tf.Tensor(\n",
      "[[0.04192291]\n",
      " [0.68333982]\n",
      " [0.10010482]\n",
      " [0.15215389]], shape=(4, 1), dtype=float64) var trial 90\n",
      "tf.Tensor(\n",
      "[[ 0.85984517]\n",
      " [-0.6520072 ]\n",
      " [-1.45373186]\n",
      " [ 0.08553436]], shape=(4, 1), dtype=float64) mean trial 91\n",
      "tf.Tensor(\n",
      "[[0.04196368]\n",
      " [0.69123249]\n",
      " [0.09932909]\n",
      " [0.15252661]], shape=(4, 1), dtype=float64) var trial 91\n",
      "tf.Tensor(\n",
      "[[ 0.86034868]\n",
      " [-0.66318076]\n",
      " [-1.47173209]\n",
      " [ 0.08482225]], shape=(4, 1), dtype=float64) mean trial 92\n",
      "tf.Tensor(\n",
      "[[0.04199815]\n",
      " [0.69913509]\n",
      " [0.09861344]\n",
      " [0.15288918]], shape=(4, 1), dtype=float64) var trial 92\n",
      "tf.Tensor(\n",
      "[[ 0.86082995]\n",
      " [-0.67377356]\n",
      " [-1.4890396 ]\n",
      " [ 0.08411185]], shape=(4, 1), dtype=float64) mean trial 93\n",
      "tf.Tensor(\n",
      "[[0.04203557]\n",
      " [0.70643867]\n",
      " [0.0979131 ]\n",
      " [0.15325104]], shape=(4, 1), dtype=float64) var trial 93\n",
      "tf.Tensor(\n",
      "[[ 0.87548724]\n",
      " [-0.67437147]\n",
      " [-1.49020911]\n",
      " [ 0.08522796]], shape=(4, 1), dtype=float64) mean trial 94\n",
      "tf.Tensor(\n",
      "[[0.04160848]\n",
      " [0.71160357]\n",
      " [0.09808188]\n",
      " [0.15341625]], shape=(4, 1), dtype=float64) var trial 94\n",
      "tf.Tensor(\n",
      "[[ 0.87837641]\n",
      " [-0.61690423]\n",
      " [-1.48409955]\n",
      " [ 0.22279514]], shape=(4, 1), dtype=float64) mean trial 95\n",
      "tf.Tensor(\n",
      "[[0.04164059]\n",
      " [0.73050083]\n",
      " [0.09776407]\n",
      " [0.14208424]], shape=(4, 1), dtype=float64) var trial 95\n",
      "tf.Tensor(\n",
      "[[ 0.87933974]\n",
      " [-0.60910935]\n",
      " [-1.48433741]\n",
      " [ 0.31730428]], shape=(4, 1), dtype=float64) mean trial 96\n",
      "tf.Tensor(\n",
      "[[0.04167302]\n",
      " [0.73423096]\n",
      " [0.09787209]\n",
      " [0.13299088]], shape=(4, 1), dtype=float64) var trial 96\n",
      "tf.Tensor(\n",
      "[[ 0.87983691]\n",
      " [-0.61560423]\n",
      " [-1.50092668]\n",
      " [ 0.31787163]], shape=(4, 1), dtype=float64) mean trial 97\n",
      "tf.Tensor(\n",
      "[[0.04170273]\n",
      " [0.74264552]\n",
      " [0.09718003]\n",
      " [0.13324798]], shape=(4, 1), dtype=float64) var trial 97\n",
      "tf.Tensor(\n",
      "[[ 0.88029428]\n",
      " [-0.62237723]\n",
      " [-1.5169201 ]\n",
      " [ 0.31844407]], shape=(4, 1), dtype=float64) mean trial 98\n",
      "tf.Tensor(\n",
      "[[0.04173512]\n",
      " [0.75055742]\n",
      " [0.09651566]\n",
      " [0.13348278]], shape=(4, 1), dtype=float64) var trial 98\n",
      "tf.Tensor(\n",
      "[[ 0.88078886]\n",
      " [-0.62860631]\n",
      " [-1.53245471]\n",
      " [ 0.31899291]], shape=(4, 1), dtype=float64) mean trial 99\n",
      "tf.Tensor(\n",
      "[[0.0417554 ]\n",
      " [0.75823809]\n",
      " [0.0958286 ]\n",
      " [0.13370507]], shape=(4, 1), dtype=float64) var trial 99\n",
      "tf.Tensor(\n",
      "[[ 0.88117511]\n",
      " [-0.63459005]\n",
      " [-1.54736829]\n",
      " [ 0.31946227]], shape=(4, 1), dtype=float64) mean trial 100\n",
      "tf.Tensor(\n",
      "[[0.04179504]\n",
      " [0.76586953]\n",
      " [0.09527722]\n",
      " [0.13392809]], shape=(4, 1), dtype=float64) var trial 100\n"
     ]
    }
   ],
   "source": [
    "# for sess in range(actions.shape[0]):\n",
    "for trial in range(2,101):\n",
    "    Y = rewards[0, :trial].reshape(-1, 1)\n",
    "    X = actions[0, :trial].reshape(-1, 1)\n",
    "    mdl = gpflow.models.VGP(\n",
    "        (X, Y),\n",
    "        kernel = gpflow.kernels.SquaredExponential(),\n",
    "        likelihood = gpflow.likelihoods.Bernoulli()\n",
    "    )\n",
    "    opt = gpflow.optimizers.Scipy()\n",
    "    opt.minimize(mdl.training_loss, mdl.trainable_variables)\n",
    "    mu, var = mdl.predict_f(np.array([1,2,3,4], dtype = float).reshape(-1, 1))\n",
    "    print(mu, f\"mean trial {trial}\")\n",
    "    print(var, f\"var trial {trial}\")\n",
    "    # print(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558845b1",
   "metadata": {},
   "source": [
    "## GPyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2b5693",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GPyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf3838b",
   "metadata": {},
   "source": [
    "## block-wise fitting for all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04cbd2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dlab\\rishika_sim\\utils\\dfLoading.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sess_block'] = 0\n",
      "c:\\Users\\dlab\\rishika_sim\\utils\\dfLoading.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sess_block'] = df.groupby(['animal', 'task']).sess_block.cumsum()\n",
      "c:\\Users\\dlab\\rishika_sim\\utils\\dfLoading.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['block_group'] = df.groupby(['animal', 'task', 'session']).ngroup()\n",
      "c:\\Users\\dlab\\rishika_sim\\utils\\dfLoading.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['block_group'] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data\n"
     ]
    }
   ],
   "source": [
    "block_train_data, block_test_data = load_data_into_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0b13a431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384, 100)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_train_data['Emolga'].port.to_numpy().reshape(-1, 100).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rishika",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
