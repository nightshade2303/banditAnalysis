{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26e0d283",
   "metadata": {},
   "source": [
    "# Import and Prob functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aaeec80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.plotSettings import *\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.ndimage import uniform_filter, gaussian_filter1d\n",
    "from scipy.stats import sem, pearsonr, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3ad68a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "################# PROBABILITY FUNCTIONS ################\n",
    "def fxn(mean, arms):\n",
    "    x = np.linspace(1, arms, arms)\n",
    "    sig = 1.75/2\n",
    "#     amp = 1/(sig*np.sqrt(2*np.pi))\n",
    "    amp = 0.7\n",
    "    vo = 0.1\n",
    "    gx = (amp*np.exp(-0.5*((x-mean)**2)/(sig**2)))+vo\n",
    "#     gx = np.random.permutation(gx)\n",
    "    return gx\n",
    "\n",
    "def cauchy(median, arms):\n",
    "#     cauchy = @(x, s, t)(1./(s*pi*(1+(((x-t)./s).^2))))\n",
    "    x = np.linspace(1, arms, arms)\n",
    "    s = 1.5\n",
    "    cauchy = ((1/(s*np.pi*(1+(((x-median)/s)**2))))*3.5)+0.1;\n",
    "    return cauchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1a769872-ad1d-4657-ac6c-6c34c7115546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.46431508, 0.8       , 0.10196115, 0.15135876]), array([0.46431508, 0.8       , 0.15135876, 0.46431508]), array([0.8       , 0.15135876, 0.46431508, 0.46431508]), array([0.46431508, 0.10196115, 0.15135876, 0.8       ])] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Structured')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fxn(mean, arms, permute = False):\n",
    "    x = np.linspace(1, arms, arms)\n",
    "    sig = 1.75/2\n",
    "#     amp = 1/(sig*np.sqrt(2*np.pi))\n",
    "    amp = 0.7\n",
    "    vo = 0.1\n",
    "    gx = (amp*np.exp(-0.5*((x-mean)**2)/(sig**2)))+vo\n",
    "    if permute:\n",
    "        gx = np.random.permutation(gx)\n",
    "    return gx\n",
    "    \n",
    "fig = plt.figure(figsize = (4,6))\n",
    "# plt.figure()\n",
    "arms = 4\n",
    "\n",
    "l = [fxn(i, arms, True) for i in range(1,arms+1)]\n",
    "for ind in range(arms):\n",
    "    ax = plt.subplot(4, 1, ind+1)\n",
    "    ax.bar(np.arange(1, arms+1), l[ind], color = 'xkcd:pumpkin')\n",
    "    \n",
    "    sns.despine()\n",
    "    # ax.set_xticks(np.arange(1,arms+1), np.arange(1,arms+1))\n",
    "    ax.set_ylim(0, 0.8)\n",
    "    ax.set_yticks([0.0, 0.8], [0, 80])\n",
    "ax.set_xticks(np.arange(1,arms+1), np.arange(1,arms+1))\n",
    "print(l,'\\n')\n",
    "    # ax.set_yticks(np.arange(4)*0.25, np.arange(4)*0.25)\n",
    "#     if ind == 3:\n",
    "#         break\n",
    "# fig.supxlabel('Ports')\n",
    "# fig.supylabel('Reward probability')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# For use of \"viscm view\"\n",
    "# test_cm = parula_map\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     import numpy as np\n",
    "\n",
    "    # try:\n",
    "    #     from viscm import viscm\n",
    "    #     viscm(parula_map)\n",
    "    # except ImportError:\n",
    "    #     print(\"viscm not found, falling back on simple display\")\n",
    "    #     plt.imshow(np.linspace(0, 100, 256)[None, :], aspect='auto',\n",
    "    #                cmap=parula_map)\n",
    "    # plt.show()\n",
    "from utils.plotSettings import *\n",
    "plt.figure()\n",
    "l = [fxn(np.random.randint(1, arms+1), arms, False) for i in range(10000)]\n",
    "sns.heatmap(np.corrcoef(np.array(l).T),\n",
    "            cmap = 'winter', vmin = -1, vmax = 1, square = True,\n",
    "            xticklabels = np.arange(1, arms+1), yticklabels = np.arange(1, arms+1))\n",
    "plt.title('Structured')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84382c7a-5c90-49f1-9c52-7de2bcbf1e5a",
   "metadata": {},
   "source": [
    "# Functions for value-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6281c505",
   "metadata": {},
   "outputs": [],
   "source": [
    "################ ACTION SELECTION #######################\n",
    "# epsilon greedy action selection\n",
    "def epsilon_greedy(eps, actions, n_arms, value): \n",
    "    randn = np.random.uniform(0,1)\n",
    "    if randn <= eps:\n",
    "        action = np.random.randint(1, n_arms+1)\n",
    "    else:\n",
    "        action = np.random.choice((np.where(value == np.amax(value))[0])+1)\n",
    "    return action, 1\n",
    "\n",
    "\n",
    "# softmax action selection\n",
    "def softmax(inv_temp, actions, arms, value):\n",
    "    prob_choosing_action = np.zeros(len(arms))\n",
    "\n",
    "    for arm in range(len(arms)):\n",
    "        prob_choosing_action[arm] = (np.exp(value[arm]*inv_temp)) / np.sum(np.exp(value*inv_temp))\n",
    "\n",
    "#     actions = np.random.choice(arms, p = prob_choosing_action) ###### DO NOT USE THIS ######\n",
    "#     print(prob_choosing_action)\n",
    "    actions = np.random.multinomial(1, prob_choosing_action)\n",
    "    a = arms[actions.nonzero()[0][0]]\n",
    "    return a, prob_choosing_action\n",
    "\n",
    "def softmax_sticky(inv_temp, actions, arms, value, h, scaler):\n",
    "    prob_choosing_action = np.exp((value*inv_temp)+ (h*scaler))\n",
    "    prob_choosing_action/=np.sum(prob_choosing_action)\n",
    "    actions = np.random.multinomial(1, prob_choosing_action)\n",
    "    a = arms[actions.nonzero()[0][0]]\n",
    "    return a, prob_choosing_action\n",
    "\n",
    "# softmax biased action selection\n",
    "def softmax_biased(inv_temp, actions, arms, value, bias):\n",
    "    prob_choosing_action_biased = np.zeros(len(arms))\n",
    "\n",
    "    for arm in range(len(arms)):\n",
    "        prob_choosing_action_biased[arm] = (np.exp((value[arm]+bias[arm])*inv_temp))/(np.sum(np.exp((value+bias)*inv_temp)))\n",
    "    \n",
    "#     actions = np.random.choice(arms, p = prob_choosing_action) ###### DO NOT USE THIS ######\n",
    "#         print(prob_choosing_action_biased, arm)\n",
    "\n",
    "    actions = np.random.multinomial(1, prob_choosing_action_biased)\n",
    "    a = arms[actions.nonzero()[0][0]]\n",
    "#     print(prob_choosing_action_biased)\n",
    "    return a, prob_choosing_action_biased\n",
    "\n",
    "# softmax weighted bias action selection\n",
    "def softmax_wbias(inv_temp, actions, arms, value, wbias, w):\n",
    "    prob_choosing_action_wbias = np.zeros(len(arms))\n",
    "    \n",
    "    for arm in range(len(arms)):\n",
    "        softmax = (np.exp(value[arm]*inv_temp)) / np.sum(np.exp(value*inv_temp))\n",
    "        prob_choosing_action_wbias[arm] = (w*softmax) + ((1-w)*wbias[arm])\n",
    "    actions = np.random.multinomial(1, prob_choosing_action_wbias)\n",
    "    a = arms[actions.nonzero()[0][0]]\n",
    "\n",
    "    return a, prob_choosing_action_wbias    \n",
    "\n",
    "# Win-stay lose-shift \n",
    "def wsls(actions, reward, n_arms, shift_prob):\n",
    "    if len(actions) == 0:\n",
    "        actions = [np.random.randint(1,n_arms+1)]\n",
    "    av_actions = list(1,range(n_arms+1))\n",
    "    if reward==1:\n",
    "        action = actions[-1]\n",
    "    else:\n",
    "        shift = np.random.uniform(0,1)\n",
    "        if shift<=shift_prob:\n",
    "            av_actions.pop(actions[-1])\n",
    "            action = np.random.choice(av_actions)\n",
    "        else:\n",
    "            action = actions[-1]\n",
    "    return action, 1\n",
    "\n",
    "\n",
    "# upper confidence bound\n",
    "def ucb(c, actions, arms, value):\n",
    "    prob_choosing_action = np.zeros(len(arms))\n",
    "    nt = np.zeros(n_arms)\n",
    "    \n",
    "    for arm in range(len(arms)):\n",
    "        nt[arm] = actions.count(arm)\n",
    "        if nt[arm]!=0:\n",
    "            prob_choosing_action[arm] = value[arm] + (c*np.sqrt(np.log(len(actions)/nt[arm])))\n",
    "        else:\n",
    "            prob_choosing_action[arm] = 1\n",
    "        \n",
    "    action = np.random.choice(np.where((prob_choosing_action == np.amax(prob_choosing_action))[0]))\n",
    "    return action, prob_choosing_action\n",
    "\n",
    "\n",
    "#################### REGRET #######################\n",
    "# regret at each timestep for minimization?\n",
    "def regret(action, prob_arms):\n",
    "    reg = max(prob_arms) - prob_arms[action-1] \n",
    "    return reg\n",
    "\n",
    "################ GIVING REWARD #####################\n",
    "def rewarding(prob, reward_val):\n",
    "    temp = reward_val\n",
    "    rand = np.random.uniform(0, 1)\n",
    "    return temp if rand <= prob else 0\n",
    "\n",
    "################# VANILLA VALUE UPDATION #############\n",
    "def qlearn(value, action, alpha, reward):\n",
    "    value[int(action)-1] = value[int(action)-1] + alpha * (reward - value[int(action)-1])\n",
    "    return value \n",
    "\n",
    "################# DEVALUE OTHER ARMS #################\n",
    "def qlearnAllArms(value, action, alpha, reward):\n",
    "    for val in range(len(value)):\n",
    "        if val == (int(action)-1):\n",
    "            value[val] = value[val] + alpha[0] * (reward - value[int(action)-1])\n",
    "        else:\n",
    "            value[val] = value[val] + alpha[1] * (reward - value[int(action)-1])\n",
    "    return value\n",
    "\n",
    "\n",
    "############### BAYESIAN(?) VALUE UPDATION ###########\n",
    "def bayesQlearn(value, action, alpha, reward):\n",
    "    value[int(action)] = value[int(action)] + alpha * (reward - value[int(action)])\n",
    "    return value\n",
    "\n",
    "############## NEW MATRIX-WISE LEARNING ########### \n",
    "def qlearnAllMat(value, action, alpha, reward):\n",
    "    for i in range(len(value)):\n",
    "        value[i] = value[i] + alpha[int(action)-1, i] * (reward - value[i])\n",
    "    return value\n",
    "\n",
    "\n",
    "############# side-weighted softmax values - gaussian transformation? ##############\n",
    "def convSoftmax(inv_temp, actions, arms, value, sd):\n",
    "    prob_choosing_action = np.zeros(len(arms))\n",
    "    value = gaussian_filter1d(value, sigma = sd)\n",
    "    \n",
    "    for arm in range(len(arms)):\n",
    "        prob_choosing_action[arm] = (np.exp(value[arm]*inv_temp)) / np.sum(np.exp(value*inv_temp))\n",
    "\n",
    "    # prob_choosing_action = gaussian_filter1d(prob_choosing_action, sigma = sd)\n",
    "    actions = np.random.multinomial(1, prob_choosing_action)\n",
    "    a = arms[actions.nonzero()[0][0]]\n",
    "    return a, prob_choosing_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335d2d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_prob(pk):\n",
    "    # calc prob of actions\n",
    "    unique, counts = np.unique(np.array(pk), return_counts =True)\n",
    "    outcomes = len(pk)\n",
    "    return counts/outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd808271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphas = np.linspace(0,1, num=5)\n",
    "# taus = np.logspace(-2,2, num=5)\n",
    "# cs = np.linspace(0,0.5, num=5)\n",
    "# fig = plt.figure(figsize = (15, 10))\n",
    "# ind=1\n",
    " \n",
    "# for i, alpha in enumerate(alphas):\n",
    "#     for j, temperature in enumerate(taus):\n",
    "\n",
    "############### ENVIRONMENT #######################\n",
    "np.random.seed(4231)\n",
    "n_arms = 4\n",
    "arms = list(range(1,n_arms+1))\n",
    "\n",
    "prob_arms = np.ones(n_arms)\n",
    "rew_val = np.ones(n_arms)\n",
    "eps = 0.2\n",
    "alpha = 0.1\n",
    "# acc to fit params - mean of all alpha and tau fitted = [0.09339148, 0.18939606] # 20250123\n",
    "gamma = 0.2\n",
    "c = 0.1\n",
    "shift_prob = 1\n",
    "\n",
    "# temperature = 0.18939606\n",
    "tau = 0.1\n",
    "inv_temp = 1/tau\n",
    "sd = 0.8\n",
    "# bias = [0., 0, 0.1, 0]\n",
    "# wbias = np.array([0.1, 0.5, 0.3, 0.1])\n",
    "# sum(wbias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a37788dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e84556b1c0>]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# w=0.5\n",
    "\n",
    "trials = 100\n",
    "sessions = 1000\n",
    "window = 5\n",
    "\n",
    "sess_mean_list = []\n",
    "reward_hist = {}\n",
    "rew_prob = {}\n",
    "value_hist = {}\n",
    "action_hist = {}\n",
    "corrcoef_hist = {}\n",
    "regret_hist = {}\n",
    "lls= []\n",
    "df = pd.DataFrame()\n",
    "df['trial'] = \" \"\n",
    "df['action'] = \" \"\n",
    "df['reward'] = \" \"\n",
    "df['session'] = \" \"\n",
    "df['rewprob'] = \" \"\n",
    "df['regret'] = \" \"\n",
    "\n",
    "\n",
    "################## run here ##############\n",
    "chance_level_sess = []\n",
    "# h = np.zeros(n_arms)\n",
    "\n",
    "\n",
    "# x0 = np.array([0.15387731, 0.08234891, 0.0685517 , 0.05269941, 0.28824557, 0.33129077])\n",
    "# x0 = np.array([0.15, -0.01, -0.08, -0.085, 0.06]) # structured\n",
    "# x0 = np.array([0.17, -0.07, -0.04, -0.05, 0.07]) # unstructured\n",
    "# x0 = np.array([0.3, -0.1, -0.1, -0.1, 0.1])\n",
    "# x0 = np.array([0.3, 0.1, -0.1, -0.2, 0.1, 0.25, 1])\n",
    "# alpha_diag, alpha_1diag, alpha_2diag, alpha_3diag, tau, sticky, scaler = x0\n",
    "x0 = np.array([0.2, 0.0, 0, 0])\n",
    "alpha_diag, alpha_1diag, alpha_2diag, alpha_3diag = x0\n",
    "alpha = np.array([[alpha_diag, alpha_1diag, alpha_2diag, alpha_3diag],\n",
    "                      [alpha_1diag, alpha_diag, alpha_1diag, alpha_2diag],\n",
    "                      [alpha_2diag, alpha_1diag, alpha_diag, alpha_1diag],\n",
    "                      [alpha_3diag, alpha_2diag, alpha_1diag, alpha_diag]])\n",
    "inv_temp = 1/tau\n",
    "value_arr = np.zeros((sessions, trials, n_arms))\n",
    "\n",
    "for sess in range(sessions):\n",
    "\n",
    "    actions = []\n",
    "    if sess%3==0:\n",
    "        q0 = 0.25*np.ones(n_arms)\n",
    "    # h = np.zeros(n_arms)\n",
    "    value=np.copy(q0)\n",
    "    \n",
    "    sess_mean = np.random.randint(1,n_arms+1) # check randint docs for details - returns number between 1 and 8\n",
    "    sess_mean_list.append(sess_mean)\n",
    "    gx = fxn(sess_mean, n_arms, True)\n",
    "    # l = [fxn(np.random.randint(1, n_arms+1), n_arms, False) for i in range(10000)]\n",
    "    # alpha = np.corrcoef(np.array(l).T)*0.1\n",
    "#     median = np.random.randint(1,n_arms+1)\n",
    "#      gx = cauchy(median, n_arms)\n",
    "\n",
    "    prob_arms = np.random.permutation(np.copy(gx))\n",
    "    rew_prob[sess] = prob_arms\n",
    "#     rew_val = np.copy(gx)\n",
    "\n",
    "    chance_level_sess.append(np.mean(prob_arms, axis = 0))\n",
    "    rew_temp = []\n",
    "    value_temp = []\n",
    "    corrcoef = []\n",
    "    regrets = []\n",
    "        \n",
    "    reward = 0\n",
    "    \n",
    "    for trial in range(trials):\n",
    "#         action, p = epsilon_greedy(eps, actions, n_arms, value)\n",
    "#         action, p = wsls(actions, reward, n_arms, shift_prob)\n",
    "#         action, p = ucb(c, actions, arms, value)\n",
    "        action, p = softmax(inv_temp, actions, arms, value)\n",
    "        # action, p = softmax_sticky(inv_temp, actions, arms, value, h, scaler)\n",
    "#         action, p = softmax_biased(inv_temp, actions, arms, value, bias)\n",
    "#         action, p = softmax_wbias(inv_temp, actions, arms, value, wbias, w)\n",
    "        \n",
    "        actions.append(action)\n",
    "        reg = regret(action, prob_arms)\n",
    "        regrets.append(reg)\n",
    "        reward = rewarding(prob_arms[int(action)-1], rew_val[int(action)-1])\n",
    "        rew_temp.append(reward)\n",
    "        df.loc[len(df.index)] = [trial, action, reward, sess, prob_arms[int(action)-1], reg]\n",
    "        # value = qlearn(value, action, alpha, reward)\n",
    "        # value = qlearnAllArms(value, action, alpha, reward)\n",
    "        value = qlearnAllMat(value, action, alpha, reward)\n",
    "        # add perseverative term to chosen arm\n",
    "        chosen = np.zeros(n_arms)\n",
    "        chosen[int(action)-1] = 1\n",
    "        # h = h + sticky*(chosen - h)\n",
    "        \n",
    "        # save values!!\n",
    "        value_arr[sess, trial] = value\n",
    "    # value_temp.append(value)\n",
    "    # value_hist[sess] = value_temp\n",
    "    regret_hist[sess] = regrets\n",
    "    \n",
    "plt.plot(df.groupby('session')['action'].get_group(0), 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ac6dc5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dlab\\AppData\\Local\\Temp\\ipykernel_14284\\918659957.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['shift_t0'] = (df['choice_t1']==df['action']).replace({True: 0, False: 1})\n",
      "C:\\Users\\dlab\\AppData\\Local\\Temp\\ipykernel_14284\\918659957.py:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['shift_t1'] = (df['choice_t2']==df['action']).replace({True: 0, False: 1})\n"
     ]
    }
   ],
   "source": [
    "df['choice_t1'] = df.groupby('session').action.shift(-1)\n",
    "df['choice_t2'] = df.groupby('session').action.shift(-2)\n",
    "df['shift_t0'] = (df['choice_t1']==df['action']).replace({True: 0, False: 1})\n",
    "df['shift_t1'] = (df['choice_t2']==df['action']).replace({True: 0, False: 1})\n",
    "# df['rr'] = (df.groupby('session', as_index = False)\n",
    "#             .reward\n",
    "#             .rolling(window, center=True)\n",
    "#             .mean()\n",
    "#             .reward)\n",
    "# from utils.supplementaryFunctions import calc_prob\n",
    "# df['entropy'] = (df.groupby('session', as_index = False)\n",
    "#                      .action\n",
    "#                      .rolling(window, center=True)\n",
    "#                      .apply(lambda x: entropy(calc_prob(x), base = 2))\n",
    "#                      .action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81690287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(dataset, hist = 20, trialsinsess = 100, head = False):\n",
    "    dataset = dataset.groupby(['session']).filter(lambda x: x.reward.size >= trialsinsess)\n",
    "    dataset['ct0'] = dataset.action.values\n",
    "    for i in range(1,hist): \n",
    "        dataset['ct'+str(i)] = dataset.groupby(['session']).action.shift(i) #previous action\n",
    "        dataset['shift_t'+str(i-1)] = dataset['ct'+str(i)]==dataset['ct'+str(i-1)]\n",
    "        dataset['shift_t'+str(i-1)] = dataset['shift_t'+str(i-1)].replace({True: 0, False: 1})\n",
    "        dataset['rt'+str(i)] = dataset.groupby(['session']).reward.shift(i) # previous reward\n",
    "        dataset['rt'+str(i)] = dataset['rt'+str(i)]#.replace({0:-1})\n",
    "#         dataset['choice_t'+str(i)] = dataset['choice_t'+str(i)].replace({1:'a', 2:'b', 3:'c', 4:'d'})\n",
    "    dataset = dataset.dropna()\n",
    "    if head == True:\n",
    "        dataset = dataset.groupby(['session']).head(trialsinsess)\n",
    "\n",
    "    return dataset\n",
    "hist = 6\n",
    "data = data_prep(df, hist = hist, trialsinsess = 100, head = True)\n",
    "\n",
    "q = value_arr.reshape(df.shape[0], 4)\n",
    "r = data.reward.to_numpy()\n",
    "r1 = data.rt1.to_numpy()\n",
    "r2 = data.rt2.to_numpy()\n",
    "r3 = data.rt3.to_numpy()\n",
    "r4 = data.rt4.to_numpy()\n",
    "#  get rpe\n",
    "a = (data.action.to_numpy(dtype=int))-1\n",
    "rpe = r - q[data.index, a]\n",
    "\n",
    "data['rpe'] = rpe\n",
    "\n",
    "# make lin reg model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "d1 = np.abs(data.action - data.ct1)\n",
    "d2 = np.abs(data.action - data.ct2)\n",
    "d3 = np.abs(data.action - data.ct3)\n",
    "coefs = np.zeros((4, 3))\n",
    "intercepts = np.zeros((4, 3))\n",
    "for trial_lag in range(3):\n",
    "    r_arr = [r1, r2, r3]\n",
    "    d_arr = [d1, d2, d3]\n",
    "    for dist in range(4):\n",
    "        y = rpe[(d_arr[trial_lag]==dist)]\n",
    "        X = r_arr[trial_lag][(d_arr[trial_lag]==dist)].reshape(-1, 1)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        reg = LinearRegression(fit_intercept = True).fit(X_train, y_train)\n",
    "        coefs[dist, trial_lag] = reg.coef_[0]\n",
    "        print(reg.coef_, dist, trial_lag)\n",
    "        intercepts[dist, trial_lag] = reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0236178",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(coefs.T, 'o-')\n",
    "plt.xticks([0, 1, 2], ['t-1', 't-2', 't-3'])\n",
    "plt.legend([0, 1, 2, 3], title = 'distance')\n",
    "plt.xlabel('trial history')\n",
    "plt.ylabel('coefficient between reward t-n and rpe (slope)')\n",
    "plt.title(f'RPE{[alpha_diag, alpha_1diag, alpha_2diag, alpha_3diag, tau, sticky, scaler]}')\n",
    "# sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "62b9faa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dlab\\AppData\\Local\\Temp\\ipykernel_14284\\3627294689.py:9: FutureWarning: The previous implementation of stack is deprecated and will be removed in a future version of pandas. See the What's New notes for pandas 2.1.0 for details. Specify future_stack=True to adopt the new implementation and silence this warning.\n",
      "  .stack().groupby(level=0)\n",
      "C:\\Users\\dlab\\AppData\\Local\\Temp\\ipykernel_14284\\3627294689.py:9: FutureWarning: The previous implementation of stack is deprecated and will be removed in a future version of pandas. See the What's New notes for pandas 2.1.0 for details. Specify future_stack=True to adopt the new implementation and silence this warning.\n",
      "  .stack().groupby(level=0)\n",
      "C:\\Users\\dlab\\AppData\\Local\\Temp\\ipykernel_14284\\3627294689.py:28: SmallSampleWarning: After omitting NaNs, one or more axis-slices of one or more sample arguments is too small; corresponding elements of returned arrays will be NaN. See documentation for sample size requirements.\n",
      "  rr_sem = sem(rr_mat, nan_policy = 'omit')\n",
      "C:\\Users\\dlab\\AppData\\Local\\Temp\\ipykernel_14284\\3627294689.py:9: FutureWarning: The previous implementation of stack is deprecated and will be removed in a future version of pandas. See the What's New notes for pandas 2.1.0 for details. Specify future_stack=True to adopt the new implementation and silence this warning.\n",
      "  .stack().groupby(level=0)\n",
      "C:\\Users\\dlab\\AppData\\Local\\Temp\\ipykernel_14284\\3627294689.py:37: SmallSampleWarning: After omitting NaNs, one or more axis-slices of one or more sample arguments is too small; corresponding elements of returned arrays will be NaN. See documentation for sample size requirements.\n",
      "  entropy_sem = sem(entropy_mat, nan_policy = 'omit')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Transition matrix')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# potentially plot everything for this model, rr, entropy, tm, regret, distance, bias analysis, variability \n",
    "%matplotlib qt\n",
    "fig = plt.figure(figsize = (10, 7))\n",
    "\n",
    "def avg_mat(df, col):\n",
    "    g = df.groupby('session').cumcount()\n",
    "    L = np.array(df.set_index(['session',g])\n",
    "           .unstack(fill_value=0)\n",
    "           .stack().groupby(level=0)\n",
    "           .apply(lambda x: x[col].values.tolist())\n",
    "           .tolist())\n",
    "    return L\n",
    "\n",
    "\n",
    "# figure 1 - regret across all sessions\n",
    "ax = plt.subplot(221)\n",
    "reg_mat = avg_mat(df, 'regret')\n",
    "reg_mean = np.mean(reg_mat, axis = 0)\n",
    "reg_sem = sem(reg_mat, nan_policy = 'omit')\n",
    "ax.plot(reg_mean, color = 'xkcd:azure')\n",
    "ax.fill_between(np.arange(reg_mat.shape[1]), reg_mean - reg_sem, reg_mean + reg_sem,  color = 'xkcd:azure', alpha = 0.2)\n",
    "ax.set_title('Regret')\n",
    "\n",
    "# figure 2 - performance plot across all sessions\n",
    "ax = plt.subplot(222)\n",
    "rr_mat = avg_mat(df, 'rr')\n",
    "rr_mean = np.mean(rr_mat, axis = 0)\n",
    "rr_sem = sem(rr_mat, nan_policy = 'omit')\n",
    "ax.plot(rr_mean, color = 'xkcd:azure')\n",
    "ax.fill_between(np.arange(rr_mat.shape[1]), rr_mean - rr_sem, rr_mean + rr_sem,  color = 'xkcd:azure', alpha = 0.2)\n",
    "ax.set_title('Performance - reward rate')\n",
    "\n",
    "# figure 3 - entropy plot across all sessions\n",
    "ax = plt.subplot(223)\n",
    "entropy_mat = avg_mat(df, 'entropy')\n",
    "entropy_mean = np.mean(entropy_mat, axis = 0)\n",
    "entropy_sem = sem(entropy_mat, nan_policy = 'omit')\n",
    "ax.plot(entropy_mean, color = 'xkcd:azure')\n",
    "ax.fill_between(np.arange(entropy_mat.shape[1]), entropy_mean - entropy_sem,\n",
    "                 entropy_mean + entropy_sem,  color = 'xkcd:azure', alpha = 0.2)\n",
    "ax.set_title('Entropy')\n",
    "\n",
    "sns.despine()\n",
    "from utils.plotSettings import *\n",
    "parula = get_parula_cmap()\n",
    "# figure 4 - transition matrix\n",
    "ax = plt.subplot(224)\n",
    "sns.heatmap(pd.crosstab(df.action, df.choice_t1, normalize = 'index'),\n",
    "            cmap = parula, annot = True, fmt = '.2f', vmin = 0.0, vmax = 0.7, #mask = np.eye(4),\n",
    "            xticklabels = np.arange(1,5), yticklabels = np.arange(1,5), ax = ax)\n",
    "# ax.patch.set_facecolor('white')\n",
    "ax.set_title('Transition matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485f043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.heatmap(alpha, cmap = parula_map, annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c71db8d7-2d4a-485f-beba-056ea79d8ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.heatmap(pd.crosstab(df.action, df.choice_t1, normalize = 'index'), square = True,\n",
    "            cmap = parula, vmin = 0, vmax = 0.5, xticklabels=np.arange(1,5), yticklabels= np.arange(1,5))\n",
    "plt.xlabel('Choice at t+1')\n",
    "plt.ylabel('Choice at t')\n",
    "plt.tight_layout()\n",
    "# ax.patch.set_facecolor('white')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f5e6d5-b67a-4259-beca-994ed0adefdb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Gradient bandit algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43618d8-67a5-49ff-9c0c-9be6c4e0e119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup env \n",
    "arms = 4\n",
    "alpha = 0.1\n",
    "trials = 100\n",
    "sessions = 1\n",
    "rew_val = 1\n",
    "# prob_arms = fxn(2, 4)\n",
    "\n",
    "# parameterized policy \n",
    "def policy(a, theta_arm):\n",
    "    return (np.exp(theta_arm[a])/ np.sum(np.exp(theta_arm)))\n",
    "\n",
    "# update policy\n",
    "for session in range(sessions):\n",
    "    # initialize\n",
    "    rr = np.zeros(trials)\n",
    "    theta_arm = np.zeros(arms)\n",
    "    del_theta = np.zeros(arms)\n",
    "    R_hist = np.zeros(trials)\n",
    "    mean_p = np.random.randint(1, 5)\n",
    "    prob_arms = fxn(mean_p, arms)\n",
    "    \n",
    "    for trial in range(trials):\n",
    "        chosen = np.random.multinomial(1, [policy(a, theta_arm) for a in range(arms)]).nonzero()[0][0]\n",
    "    \n",
    "        R = rewarding(prob_arms[chosen], rew_val)\n",
    "        \n",
    "        # R_hist[trial] = R\n",
    "    \n",
    "        rr[trial] = np.nanmean(R_hist)\n",
    "    \n",
    "        del_theta = [(alpha*(1-policy(a, theta_arm))*(R - rr[trial])) \n",
    "                     if (chosen == a) \n",
    "                     else (-alpha*(policy(a, theta_arm))*(R - rr[trial])) for a in range(arms)]\n",
    "            \n",
    "        theta_arm = [(theta_arm[a] + del_theta[a]) for a in range(arms)]\n",
    "        \n",
    "        print(trial, R, chosen, theta_arm[chosen])\n",
    "\n",
    "        R_hist[trial] = R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48604ef2",
   "metadata": {},
   "source": [
    "# Thompson sampling variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a9ce4b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "from scipy.stats import beta\n",
    "trials = 100\n",
    "sessions = 1000\n",
    "arms = 4\n",
    "\n",
    "a = np.zeros((sessions, trials))\n",
    "r = np.zeros((sessions, trials))\n",
    "p_est = np.zeros((sessions, trials, arms), dtype = float)\n",
    "rp_set = [fxn(np.random.randint(1, arms+1), arms, True) for i in range(10000)]\n",
    "w_diag, w_1diag, w_2diag, w_3diag = 0.95, 0.8, 0.7, 0.6\n",
    "w = np.array([[w_diag, w_1diag, w_2diag, w_3diag],\n",
    "                      [w_1diag, w_diag, w_1diag, w_2diag],\n",
    "                      [w_2diag, w_1diag, w_diag, w_1diag],\n",
    "                      [w_3diag, w_2diag, w_1diag, w_diag]])\n",
    "alpha0 = 1\n",
    "beta0 = 1\n",
    "alphas = np.ones(arms)*alpha0\n",
    "betas = np.ones(arms)*beta0\n",
    "# samples = np.zeros((arms, 500))\n",
    "\n",
    "# c = 2\n",
    "# w = 1\n",
    "for sess in range(sessions):\n",
    "    # reward probability changes here\n",
    "    rp = rp_set[sess]\n",
    "    alphas = np.ones(arms)*alpha0\n",
    "    betas = np.ones(arms)*beta0\n",
    "    # if sess%3==0:\n",
    "    #     alphas = np.ones(arms)*alpha0\n",
    "    #     betas = np.ones(arms)*beta0\n",
    "    for t in range(trials):\n",
    "        # draw mean of distribution from s+ alpha, f+beta\n",
    "        # p_est[sess, t, arm] = (alphas[arm])/(alphas[arm]+betas[arm])   # beta prior\n",
    "        # p_est[sess, t, arm] = ((alphas[arm]+alpha0)/(alphas[arm]+alpha0+betas[arm]+beta0))+(beta.std(alphas[arm]+alpha0, betas[arm]+beta0)*c)       # beta prior mean+ c*ucb\n",
    "        samples = np.random.beta(alphas, betas, size = (500, arms))  # trying to get var estimate\n",
    "        # p_est[sess, t, arm] = np.random.beta(alphas[arm]+alpha0, betas[arm]+beta0, size = 500)      # actual thompson sampling\n",
    "\n",
    "        # draw arm using samples drawn from distr\n",
    "        arm_choices = np.argmax(samples, axis=1)\n",
    "        counts = np.bincount(arm_choices, minlength=arms)\n",
    "        p_est[sess, t, :] = counts / counts.sum()\n",
    "        # p_est[sess, t, :] = np.clip(p_est[sess, t, :], a_min=1e-6, a_max = 1)\n",
    "\n",
    "        # a[sess, t] = np.random.choice(np.where(p_est[sess, t] == np.amax(p_est[sess, t]))[0])\n",
    "        a[sess, t] = np.random.choice(np.arange(arms), p = p_est[sess, t, :])\n",
    "        chosen = int(a[sess, t])\n",
    "        # reward chosen arm\n",
    "        r[sess, t] = rewarding(rp[int(a[sess, t])], 1)\n",
    "        \n",
    "        alphas = np.array([(alphas[arm]*w[arm, chosen]) for arm in range(arms)])\n",
    "        betas = np.array([(betas[arm]*w[arm, chosen]) for arm in range(arms)])\n",
    "        # alphas = np.clip(alphas, a_min=1, a_max = None)\n",
    "        # betas = np.clip(betas, a_min = 1, a_max = None)\n",
    "        alphas[chosen] = alphas[chosen]+r[sess, t]+alpha0\n",
    "        betas[chosen] = betas[chosen]+(1-r[sess, t])+beta0\n",
    "        \n",
    "        # alphas[chosen] = (w*alphas[chosen])+r[sess, t]+alpha0\n",
    "        # betas[chosen] = (w*betas[chosen])+(1-r[sess, t])+beta0\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b233f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ashesh's modified thompson sampling with learning rate l and forgetting rate w\n",
    "alpha0 = 1\n",
    "beta0 = 1\n",
    "w = 0.95\n",
    "l = 1\n",
    "for sess in range(sessions):\n",
    "    # reward probability changes here\n",
    "    rp = rp_set[sess]\n",
    "    if sess%3==0:\n",
    "        alphas = np.ones(arms)*alpha0\n",
    "        betas = np.ones(arms)*beta0\n",
    "        s = np.zeros(arms)\n",
    "        f = np.zeros(arms)\n",
    "\n",
    "    for t in range(trials):\n",
    "        # draw samples from distribution\n",
    "        samples = np.random.beta(alphas, betas, size = (20, arms))  # trying to get var estimate\n",
    "\n",
    "        # calc prob using samples\n",
    "        arm_choices = np.argmax(samples, axis=1)\n",
    "        counts = np.bincount(arm_choices, minlength=arms)\n",
    "        p_est[sess, t, :] = counts / counts.sum()\n",
    "\n",
    "        # select an arm using the probabilities\n",
    "        a[sess, t] = np.random.choice(np.arange(arms), p = p_est[sess, t, :])\n",
    "        chosen = int(a[sess, t])\n",
    "\n",
    "        # reward chosen arm\n",
    "        r[sess, t] = rewarding(rp[int(a[sess, t])], 1)\n",
    "\n",
    "        # # update success/fail for all arms\n",
    "        s = np.array([(w*x)+(l*r[sess, t]) if i==chosen else w*x for i, x in enumerate(s)])\n",
    "        f = np.array([(w*x)+(l*(1-r[sess, t])) if i==chosen else w*x for i, x in enumerate(f)])\n",
    "\n",
    "        # linked parameters\n",
    "        # s = np.array([(w*x) + (l_plus[i, chosen]*r[sess, t])     + (l_minus[i, chosen]*(1-r[sess, t])) for i, x in enumerate(s)])\n",
    "        # f = np.array([(w*x) + (l_plus[i, chosen]*(1-r[sess, t])) + (l_minus[i, chosen]*r[sess, t]) for i, x in enumerate(f)])\n",
    "\n",
    "        # update alpha and beta\n",
    "        alphas = alpha0 + s\n",
    "        betas = beta0 + f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a686f8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_diag, w_1diag, w_2diag, w_3diag = 1, 0.95, 0.85, 0.7\n",
    "l_plus = np.array([[w_diag, w_1diag, w_2diag, w_3diag],\n",
    "                      [w_1diag, w_diag, w_1diag, w_2diag],\n",
    "                      [w_2diag, w_1diag, w_diag, w_1diag],\n",
    "                      [w_3diag, w_2diag, w_1diag, w_diag]])\n",
    "w_diag, w_1diag, w_2diag, w_3diag = 0, 0.9, 0.8, 0.7\n",
    "l_minus = np.array([[w_diag, w_1diag, w_2diag, w_3diag],\n",
    "                      [w_1diag, w_diag, w_1diag, w_2diag],\n",
    "                      [w_2diag, w_1diag, w_diag, w_1diag],\n",
    "                      [w_3diag, w_2diag, w_1diag, w_diag]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7f6b0cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot behavior scatter in random subset of sessions\n",
    "plt.plot(a[9:12].flatten(), '.', color = 'xkcd:cornflower')\n",
    "plt.fill_between(np.arange(300),\n",
    "                 np.repeat(np.argmax(rp_set[9:12], axis = 1)-0.5, 100), \n",
    "                 np.repeat(np.argmax(rp_set[9:12], axis = 1)+0.5, 100), \n",
    "                 alpha = 0.2, color = 'xkcd:cornflower')\n",
    "plt.yticks([0, 1, 2, 3], [1,2,3,4])\n",
    "plt.xlabel('Trials')\n",
    "plt.ylabel('Choices')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "74062844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition matrix\n",
    "\n",
    "parula = get_parula_cmap()\n",
    "sns.heatmap(pd.crosstab(pd.Series(a.flatten()).shift(-1), a.flatten(), normalize= 'index'),\n",
    "            cmap = parula,\n",
    "            square=True, annot=True,\n",
    "            vmax = 0.25,\n",
    "            xticklabels=np.arange(1,5), \n",
    "            yticklabels=np.arange(1,5))\n",
    "plt.xlabel('Choice at trial t+1')\n",
    "plt.ylabel('Choice at trial t')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e265b969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.55555556 0.5        0.5        0.6       ]\n",
      "[0.09936019 0.40824829 0.40824829 0.2773501 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.22592438, 0.17653742, 0.17653742, 0.42100078])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas = np.array([25, 1, 1, 3])\n",
    "betas = np.array([20, 1, 1, 2])\n",
    "# draw expectation of each arm being selected (alp/ alp+beta)\n",
    "q = alphas/(alphas+betas)\n",
    "ucb = np.sqrt((alphas*betas)/(((alphas+betas)**2)*alphas+betas+np.ones(arms)))\n",
    "print(q)\n",
    "print(ucb)\n",
    "# softmax prob of choosing actions\n",
    "invtemp=1/tau\n",
    "P = np.exp(invtemp*(q+c*ucb))\n",
    "P = P/ np.sum(P) \n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "173458cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward rate\n",
    "# plt.figure()\n",
    "plt.plot(np.mean(r, axis = 0), color = 'xkcd:cornflower')\n",
    "plt.fill_between(np.arange(trials), np.mean(r, axis = 0) - sem(r, axis = 0), np.mean(r, axis = 0) + sem(r, axis = 0), alpha = 0.3, color = 'xkcd:cornflower')\n",
    "chance_level = np.mean(rp_set)\n",
    "plt.axhline(chance_level, linestyle = '--', color = 'k')\n",
    "plt.xlabel('Trials in session')\n",
    "plt.ylabel('Reward rate')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ab4d0be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate switch-rate (trial-wise?)\n",
    "# get all first blocks\n",
    "switches = (np.diff(a, axis = 1)!=0).astype(int)\n",
    "plt.plot(np.mean(np.concatenate([switches[range(0, 999, 3), :], switches[range(1, 1000, 3), :], switches[range(2, 1000, 3), :]], axis = 1), axis = 0), '.-', \n",
    "         color = 'xkcd:cornflower')\n",
    "plt.axvline(99.5, color =  'grey', lw = 0.75, linestyle = '--')\n",
    "plt.axvline(198.5, color =  'grey', lw = 0.75, linestyle = '--')\n",
    "# plt.legend()\n",
    "plt.ylabel('Switch rate')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903cc018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate switch-distance (trial-wise?)\n",
    "# get all first blocks\n",
    "switches = np.abs(np.diff(a, axis = 1))\n",
    "# switches[switches==0]=np.nan\n",
    "plt.plot(np.mean(np.concatenate([switches[range(0, 999, 3), :], switches[range(1, 1000, 3), :], switches[range(2, 1000, 3), :]], axis = 1), axis = 0), '.-', \n",
    "         color = 'xkcd:cornflower')\n",
    "plt.axvline(99.5, color =  'grey', lw = 0.75, linestyle = '--')\n",
    "plt.axvline(198.5, color =  'grey', lw = 0.75, linestyle = '--')\n",
    "# plt.legend()\n",
    "plt.ylabel('Switch distance')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a627ccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many times the algo was wrong \n",
    "chosen_a = [np.argmax(p_est[sess, 99, :])- np.argmax(rp_set[sess]) for sess in range(sessions)]\n",
    "sns.histplot(chosen_a, discrete = True, color = 'xkcd:cornflower')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Disp. by which wrong')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5655b4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(beta.ppf(0.01, 10, 10),\n",
    "                beta.ppf(0.99, 10, 10), 100)\n",
    "plt.figure()\n",
    "for ax_n in range(arms):\n",
    "    plt.subplot(1, arms, ax_n+1)\n",
    "    rv = beta(alphas[ax_n], betas[ax_n])\n",
    "    plt.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
    "    # plt.xlim(0,1)\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rishika",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
